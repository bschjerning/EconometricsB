{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from scipy.stats import chi2\n",
    "from tabulate import tabulate\n",
    "import LinearModelsWeek3_post as lm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x, t, year, label_y, label_x = lm.load_example_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set introduction\n",
    "On Tuesday we briefly discussed how the presence of fixed effects causes the estimator to be biased. To recap, consider the following model,\n",
    "\n",
    "$$ y_{it} = \\boldsymbol{x}_{it}\\boldsymbol{\\beta} + c_i + u_{it}, \\quad i=1, \\dotsc, N \\quad t=1, \\dotsc, T \\tag{1} $$\n",
    "\n",
    "where $c_i$ is an unobservable individual specific component which is constant across time. We consider two different scenarios: \n",
    "\n",
    "* **Part 1:** If $c$ is systematically related to one or more of the observed variables in the sense of $E[c_{i}\\boldsymbol{x}_{it}] \\neq \\boldsymbol{0}$, then the POLS estimator is _not_ consistent for $\\boldsymbol{\\beta}$.\n",
    "* **Part 2:*** If $c_i$ is uncorrelated with the regressors such that $E[c_i\\boldsymbol{x}_{it}]=0$ for all $t$, then $\\boldsymbol{\\beta}$ can be consistently estimated by pooled OLS (POLS) and random effects (RE). \n",
    "\n",
    "### Example\n",
    "Let's take a look at a proper example. We are interested in the effect of unionization on wage, this could be modelled as such.\n",
    "\n",
    "$$\n",
    "\\ln(wage_{it}) = \\beta_0 + \\beta_1\\textit{union} + c_{i} + u_{it} \\tag{2}\n",
    "$$\n",
    "\n",
    "Consider what could be in $c_i$ that may be correlated with unionizing? Let us first calcualte what the average union participation is, by checking the mean of the union variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 24.40% of our sample is in an union.\n"
     ]
    }
   ],
   "source": [
    "mean_union = x[:, -1].mean()\n",
    "print(f'About {mean_union * 100:.2f}% of our sample is in an union.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some fixed effects that we could control for, for example if we believe afro americans are more or less prone to unionizing, because of some social economic factors. We can look at the conditional mean for afro americans and hispanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we look at the unionization of some sub populations, afro american membership is at 37.10%, hispanic membership is at 27.35%.\n"
     ]
    }
   ],
   "source": [
    "black_union = x[x[:, 1] == 1, -1].mean()\n",
    "hispanic_union = x[x[:, 2] == 1, -1].mean()\n",
    "print(f'If we look at the unionization of some sub populations, afro american membership is at {black_union * 100:.2f}%, hispanic membership is at {hispanic_union * 100:.2f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethnicity may therefore be a fixed effect which is systematically related to $\\textit{union}$ (again, most likely ethnicity does not affect union, but it might be a proxy for some socio-economic factors that affect union membership). In our data, this is something which we can control for, by including it in our regression.\n",
    "\n",
    "We therefore consider the somewhat more elaborate model from Tuesday,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ln\\left(wage_{it}\\right) & =\\beta_{0}+\\beta_{1}\\textit{exper}_{it}+\\beta_{2}\\textit{exper}_{it}^{2}+\\beta_{3}\\textit{union}_{it}+\\beta_{4}\\textit{married}_{i}\\nonumber \\\\\n",
    " & \\quad+\\beta_{5}\\textit{educ}_{i}+\\beta_{6}\\textit{hisp}_{i}+\\beta_{7}\\textit{black}_{i}+c_{i}+u_{it}. \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This should solve some of our problems compared to eq. (2), but we still have an issue if for example people select into union or non-union jobs based on which sector rewards their innate characteristics best, then $E[uniont_{it}c_i]\\neq0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Compare POLS to FE/FD\n",
    "### Question 1:\n",
    "\n",
    "Start by estimating eq. (3) by POLS. You should already have all the data and code that you need, print it out in a nice table. Is the unionization coefficient statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL IN\n",
    "# First, estimate y on x without any transformations. Store the resulting dictionary.\n",
    "# Then, print the resulting dictionary using the provided print_table() function. The labels should have been provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled OLS\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                   Beta      Se    t-values\n",
      "--------------  -------  ------  ----------\n",
      "Constant        -0.0347  0.0646     -0.5375\n",
      "Black           -0.1438  0.0236     -6.1055\n",
      "Hispanic         0.0157  0.0208      0.7543\n",
      "Education        0.0994  0.0047     21.2476\n",
      "Experience       0.0892  0.0101      8.8200\n",
      "Experience sqr  -0.0028  0.0007     -4.0272\n",
      "Married          0.1077  0.0157      6.8592\n",
      "Union            0.1801  0.0171     10.5179\n",
      "R² = 0.187\n",
      "σ² = 0.231\n"
     ]
    }
   ],
   "source": [
    "ols_result = lm.estimate(y, x)\n",
    "lm.print_table(\n",
    "    (label_y, label_x), ols_result, title=\"Pooled OLS\", floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a table that look like this:\n",
    "\n",
    "Pooled OLS <br>\n",
    "Dependent variable: Log wage <br>\n",
    "\n",
    "|                |    Beta |     Se |   t-values |\n",
    "|----------------|---------|--------|------------|\n",
    "| Constant       | -0.0347 | 0.0646 |    -0.5375 |\n",
    "| Black          | -0.1438 | 0.0236 |    -6.1055 |\n",
    "| Hispanic       |  0.0157 | 0.0208 |     0.7543 |\n",
    "| Education      |  0.0994 | 0.0047 |    21.2476 |\n",
    "| Experience     |  0.0892 | 0.0101 |     8.8200 |\n",
    "| Experience sqr | -0.0028 | 0.0007 |    -4.0272 |\n",
    "| Married        |  0.1077 | 0.0157 |     6.8592 |\n",
    "| Union          |  0.1801 | 0.0171 |    10.5179 |\n",
    "R² = 0.187 <br>\n",
    "σ² = 0.231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short introduction to fixed effects\n",
    "\n",
    "A solution to control for fixed effects, is to \"demean\" the data. We need to calculate the mean within each person, so we define $\\bar{y}_{i}=T^{-1}\\sum_{t=1}^{T}y_{it}$, $\\mathbf{\\bar{x}}_{i}=T^{-1}\\sum_{t=1}^{T}\\mathbf{x}_{it}$, $\\mathbf{\\bar{u}}_{i}=T^{-1}\\sum_{t=1}^{T}\\mathbf{u}_{it}$, and $c_{i} = T^{-1}\\sum_{t=1}^{T}c_{i}$.\n",
    "\n",
    "Subtracting these means from eq. (1) we are able to demean away the fixed effects,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{it}-\\bar{y}_{i} & =\\left(\\mathbf{x}_{it}-\\mathbf{\\bar{x}}_{i}\\right)\\mathbf{\\beta}+(\\color{red}{c_{i}-c_{i}})+\\left(u_{it}-\\bar{u}_{i}\\right) \\\\\n",
    "\\Leftrightarrow\\ddot{y}_{it} & =\\ddot{\\mathbf{x}}_{it}\\mathbf{\\beta} + \\ddot{u}_{it}. \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To substract the mean within each person is not immediately easy. But you are provided with a `perm` function, that takes a \"transformation matrix\" Q, and uses it to permutate some vector or matrix A.\n",
    "\n",
    "In order to demean the data, we need to give this `perm` function the following transformation matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Q}_{T}:=\\mathbf{I}_{T}-\\left(\\begin{array}{ccc}\n",
    "1/T & \\ldots & 1/T\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "1/T & \\ldots & 1/T\n",
    "\\end{array}\\right)_{T\\times T}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Estimate eq. (3) by fixed effects. You need to perform the following steps:\n",
    "* Create the demeaning matrix Q.\n",
    "* Demean x and y using the `perm` function and Q.\n",
    "* Remove the columns in the demeaned x that are only zeroes (remember to shorten the `label_x` as well).\n",
    "* Estimate y on x using the demeaned arrays.\n",
    "* Print it out in a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL IN\n",
    "# The steps are outlined in question 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE regression\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                   Beta      Se    t-values\n",
      "--------------  -------  ------  ----------\n",
      "Experience       0.1168  0.0084     13.8778\n",
      "Experience sqr  -0.0043  0.0006     -7.1057\n",
      "Married          0.0453  0.0183      2.4743\n",
      "Union            0.0821  0.0193      4.2553\n",
      "R² = 0.178\n",
      "σ² = 0.123\n"
     ]
    }
   ],
   "source": [
    "def demeaning_matrix(t):\n",
    "    Q_T = np.eye(t) - np.tile(1/t, (t, t))\n",
    "    return Q_T\n",
    "\n",
    "# Demean the matrices\n",
    "Q_T = demeaning_matrix(t)\n",
    "y_demean = lm.perm(Q_T, y)\n",
    "x_demean = lm.perm(Q_T, x)\n",
    "x_demean = x_demean[:, 4:]\n",
    "label_x_fe = label_x[4:]\n",
    "\n",
    "# Estimate using OLS and print the results\n",
    "fe_result = lm.estimate(\n",
    "    y_demean, x_demean, transform='fe', t=t\n",
    ")\n",
    "lm.print_table(\n",
    "    (label_y, label_x_fe), \n",
    "    fe_result, title='FE regression', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a table that look like this:\n",
    "\n",
    "FE regression<br>\n",
    "Dependent variable: Log wage\n",
    "\n",
    "|                |    Beta |     Se |   t-values |\n",
    "|----------------|---------|--------|------------|\n",
    "| Experience     |  0.1168 | 0.0084 |    13.8778 |\n",
    "| Experience sqr | -0.0043 | 0.0006 |    -7.1057 |\n",
    "| Married        |  0.0453 | 0.0183 |     2.4743 |\n",
    "| Union          |  0.0821 | 0.0193 |     4.2553 |\n",
    "R² = 0.178 <br>\n",
    "σ² = 0.123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to first differences\n",
    "\n",
    "The within transformation is one particular transformation\n",
    "that enables us to get rid of $c_{i}$. An alternative is the first-difference transformation. To see how it works, lag Equation (1) one period and subtract it from (1) such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta y_{it}=\\Delta\\mathbf{x}_{it}\\mathbf{\\beta}+\\Delta u_{it},\\quad t=\\color{red}{2},\\dotsc,T, \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Delta y_{it}:=y_{it}-y_{it-1}$, $\\Delta\\mathbf{x}_{it}:=\\mathbf{x}_{it}-\\mathbf{x}_{it-1}$ and $\\Delta u_{it}:=u_{it}-u_{it-1}$. As was the case for the within transformation, first differencing eliminates the time invariant component $c_{i}$. Note, however, that one time period is lost when differencing.\n",
    "\n",
    "In order to first difference the data, we can pass the following transformation matrix to the `perm` function,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{D}:=\\left(\\begin{array}{cccccc}\n",
    "-1 & 1 & 0 & \\ldots & 0 & 0\\\\\n",
    "0 & -1 & 1 &  & 0 & 0\\\\\n",
    "\\vdots &  &  & \\ddots &  & \\vdots\\\\\n",
    "0 & 0 & 0 & \\ldots & -1 & 1\n",
    "\\end{array}\\right)_{T - 1\\times T}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Estimate eq. (3) by first differences. You need to perform the following steps:\n",
    "* Create the first difference matrix D.\n",
    "* First difference x and y using the `perm` function and Q.\n",
    "* Remove the columns in the first differenced x that are only zeroes (remember to shorten the `label_x` as well).\n",
    "* Estimate y on x using the first differenced arrays.\n",
    "* Print it out in a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL IN\n",
    "# The steps are outlined in question 3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD regression\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                   Beta      Se    t-values\n",
      "--------------  -------  ------  ----------\n",
      "Experience       0.1158  0.0196      5.9096\n",
      "Experience sqr  -0.0039  0.0014     -2.8005\n",
      "Married          0.0381  0.0229      1.6633\n",
      "Union            0.0428  0.0197      2.1767\n",
      "R² = 0.004\n",
      "σ² = 0.196\n"
     ]
    }
   ],
   "source": [
    "# First difference the matrices\n",
    "def fd_matrix(t):\n",
    "    D_T = np.eye(t) - np.eye(t, k=-1)\n",
    "    D_T = D_T[1:]\n",
    "    return D_T\n",
    "D_T = fd_matrix(t)\n",
    "y_diff = lm.perm(D_T, y)\n",
    "x_diff = lm.perm(D_T, x)\n",
    "x_diff = x_diff[:, 4:]\n",
    "\n",
    "# Estimate using OLS and print the results\n",
    "fd_result = lm.estimate(y_diff, x_diff, transform='fd')\n",
    "lm.print_table(\n",
    "    (label_y, label_x_fe), \n",
    "    fd_result, title='FD regression', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a table that look like this:\n",
    "\n",
    "FD regression <br>\n",
    "Dependent variable: Log wage\n",
    "\n",
    "|                |    Beta |     Se |   t-values |\n",
    "|----------------|---------|--------|------------|\n",
    "| Experience     |  0.1158 | 0.0196 |     5.9096 |\n",
    "| Experience sqr | -0.0039 | 0.0014 |    -2.8005 |\n",
    "| Married        |  0.0381 | 0.0229 |     1.6633 |\n",
    "| Union          |  0.0428 | 0.0197 |     2.1767 |\n",
    "R² = 0.004 <br>\n",
    "σ² = 0.196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up part questions 1, 2, and 3.\n",
    "Compare the results from your POLS, FE and FD estimations. We were mainly interested in the effect of $\\textit{union}$ on wages, did the POLS estimation give a correct conlcusion on this? Is the effect greater or lower than we first though? Is the effect still statistically significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The random effects (RE) estimator.\n",
    "In part 1 we assumed that $E[uniont_{it}c_i]\\neq0$, and used two methods to remove these fixed effects from each person. Now, what if $E[uniont_{it}c_i] )= 0$? Then POLS is consistent, but not efficient, since POLS is not using the panel structure of the data. We can therefore do better with the RE estimator.\n",
    "\n",
    "## A short introduction to the RE estimator\n",
    "As with the FE and FD estimators, we estimate them by OLS, but by first transforming them in a specific way. We do the same now, but our mission is no longer to transform away the fixed effects, but rather estimate the following model,\n",
    "\n",
    "$$\\check{y}_{it} = \\mathbf{\\check{x}}_{it}\\mathbf{\\beta} + \\check{v}_{it},\\tag{6}$$ \n",
    "\n",
    " $\\mathbf{\\check{y}}_{it} = \\mathbf{y}_{it} - \\hat{\\lambda}\\mathbf{\\bar{y}}_{it}$, $\\mathbf{\\check{x}}_{it} = \\mathbf{x}_{it} - \\hat{\\lambda}\\mathbf{\\bar{x}}_{it}$, and $\\check{v}_{it} = v_{it} - \\hat{\\lambda}\\bar{v}_{it}$, where we have gathered the errors $v_{it} = c_i + u_{it}$. We are *\"quasi-demeaning\"* the variables, by premultiplying the means by $\\hat{\\lambda}$.\n",
    "\n",
    " Our challenge is thus to estimate this $\\lambda$, which we can construct the following way,\n",
    "\n",
    "$$\\hat{\\lambda} = 1 - \\sqrt{\\frac{\\widehat{\\sigma}_{u}^{2}}{(\\widehat{\\sigma}_{u}^{2} + T\\widehat{\\sigma}_{c}^{2})}}, $$\n",
    "\n",
    "where $\\widehat{\\sigma}_{u}^{2}$ is estimated from the fixed effects regression, and $\\hat{\\sigma}_{c}^{2} = \\hat{\\sigma}_{w}^{2} - \\frac{1}{T}\\hat{\\sigma}_{u}^{2}$. Finaly, what is $\\hat{\\sigma}_{w}^{2}$? That is the error variance from the between estimator, \n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{w}^{2} = \\frac{1}{N-K}\\left(\\bar{\\mathbf{y}} - \\mathbf{\\bar{X}}\\hat{\\mathbf{\\beta}}_{BE}\\right)^{\\prime}\\left(\\bar{\\mathbf{y}} - \\mathbf{\\bar{X}}\\hat{\\mathbf{\\beta}}_{BE}\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\beta}_{BE}$ are the between estimater coefficients. The between-groups estimator is not something we have introduced before, but is attained by regressing the time-averaged outcomes $\\overline{y}_i$ on the time-averaged regressors $\\overline{\\mathbf{x}}_i,i=1,2,\\dotsc,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: The Between Estimator\n",
    "Estimate the between groups model, which is simply the average within each each individual,\n",
    "\n",
    "$$\n",
    "\\bar{y}_{i} = \\boldsymbol{\\bar{x}}_{i}\\boldsymbol{\\beta} + c_i + \\bar{u}_{i}.\n",
    "$$\n",
    "\n",
    "So instead of demeaning, like we did in FE, we just calculate the mean with the following transformation *vector* $\\mathbf{P}_T$,\n",
    "\n",
    "\\begin{equation} \n",
    "\\mathbf{P}_T \\equiv \\left( \\frac{1}{T}, \\frac{1}{T}, ..., \\frac{1}{T} \\right)_{1 \\times T}\n",
    "\\end{equation}\n",
    "\n",
    "In order to estimate eq. (3) with the between estimator. You need to perform the following steps:\n",
    "* Create the mean matrix `P`.\n",
    "* mean `x` and `y` using the `perm` function and `P`.\n",
    "* Regress `y_mean` on `x_mean`. Note that there are $N$ rows in each, not $NT$. \n",
    "* Print it out in a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL IN\n",
    "# The steps are outlined in question 3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BE\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                   Beta      Se    t-values\n",
      "--------------  -------  ------  ----------\n",
      "Constant         0.4923  0.2210        2.23\n",
      "Black           -0.1388  0.0489       -2.84\n",
      "Hispanic         0.0048  0.0427        0.11\n",
      "Education        0.0946  0.0109        8.68\n",
      "Experience      -0.0504  0.0503       -1.00\n",
      "Experience sqr   0.0051  0.0032        1.60\n",
      "Married          0.1437  0.0412        3.49\n",
      "Union            0.2707  0.0466        5.81\n",
      "R² = 0.219\n",
      "σ² = 0.121\n"
     ]
    }
   ],
   "source": [
    "def mean_matrix(t):\n",
    "    return np.tile(1/t, (1, t))\n",
    "P_T = mean_matrix(t)\n",
    "y_mean = lm.perm(P_T, y)\n",
    "x_mean = lm.perm(P_T, x)\n",
    "be_result = lm.estimate(\n",
    "    y_mean, x_mean, transform='be')\n",
    "lm.print_table(\n",
    "    labels=(label_y, label_x), results=be_result, \n",
    "    title='BE',\n",
    "    floatfmt=['', '.4f', '.4f', '.2f']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a table that looks like this:\n",
    "\n",
    "BE <br>\n",
    "Dependent variable: Log wage\n",
    "\n",
    "|                |   Beta |     Se |   t-values |\n",
    "|----------------|--------|--------|------------|\n",
    "| Constant        |  0.4923 | 0.2210 |  2.23 | \n",
    "| Black           | -0.1388 | 0.0489 | -2.84 | \n",
    "| Hispanic        |  0.0048 | 0.0427 |  0.11 | \n",
    "| Education       |  0.0946 | 0.0109 |  8.68 | \n",
    "| Experience      | -0.0504 | 0.0503 | -1.00 | \n",
    "| Experience sqr  |  0.0051 | 0.0032 |  1.60 | \n",
    "| Married         |  0.1437 | 0.0412 |  3.49 | \n",
    "| Union           |  0.2707 | 0.0466 |  5.81 | \n",
    "R² = 0.219 <br>\n",
    "σ² = 0.119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "You should now have all the error variances that you need to calculate,\n",
    "\n",
    "$$\\hat{\\lambda} = 1 - \\sqrt{\\frac{\\widehat{\\sigma}_{u}^{2}}{(\\widehat{\\sigma}_{u}^{2} + T\\widehat{\\sigma}_{c}^{2})}}, $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64264094]]\n"
     ]
    }
   ],
   "source": [
    "sigma_u = fe_result['sigma2']\n",
    "sigma_c = be_result['sigma2'] - sigma_u/t\n",
    "_lambda = 1 - np.sqrt(sigma_u/(sigma_u + t*sigma_c))\n",
    "print(_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Now we are finaly ready to estimate eq. (3) with random effects. Since we have to use $\\hat{\\lambda}$ to quasi-demean within each individual, we again use the `perm` function. This time, we pass it the following transformation matrix,\n",
    "\n",
    "$$\n",
    "\\mathbf{C}_{T}:=\\mathbf{I}_{T} - \\hat{\\lambda}\\mathbf{P}_{T},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{P}_{T}$ is the $T \\times T$ transformation matrix we used earlier to calculate the mean of each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "# Create first the transformation matrix C\n",
    "# Use the perm function to \"quasi-demean\" x and y using C\n",
    "# Estimate RE using OLS and print a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                  Beta      Se    t-values\n",
      "--------------  ------  ------  ----------\n",
      "Constant        -0.107  0.1107       -0.97\n",
      "Black           -0.144  0.0476       -3.03\n",
      "Hispanic         0.020  0.0426        0.47\n",
      "Education        0.101  0.0089       11.36\n",
      "Experience       0.112  0.0083       13.57\n",
      "Experience sqr  -0.004  0.0006       -6.88\n",
      "Married          0.063  0.0168        3.74\n",
      "Union            0.107  0.0178        6.02\n",
      "R² = 0.178\n",
      "σ² = 0.124\n",
      "λ = 0.643\n"
     ]
    }
   ],
   "source": [
    "C_t = np.eye(t) - _lambda*mean_matrix(t)\n",
    "x_re = lm.perm(C_t, x)\n",
    "y_re = lm.perm(C_t, y)\n",
    "\n",
    "re_result = lm.estimate(\n",
    "    y_re, x_re, transform='re', t=t\n",
    ")\n",
    "lm.print_table(\n",
    "    labels=(label_y, label_x), results=re_result, _lambda=_lambda,\n",
    "    title='RE',\n",
    "    floatfmt=['', '.3f', '.4f', '.2f']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RE <br>\n",
    "Dependent variable: Log wage <br>\n",
    "\n",
    "|                |   Beta |     Se |   t-values |\n",
    "|----------------|--------|--------|------------|\n",
    "| Constant       | -0.107 | 0.1101 |      -0.97 |\n",
    "| Black          | -0.144 | 0.0473 |      -3.04 |\n",
    "| Hispanic       |  0.020 | 0.0424 |       0.48 |\n",
    "| Education      |  0.101 | 0.0089 |      11.42 |\n",
    "| Experience     |  0.112 | 0.0083 |      13.56 |\n",
    "| Experience sqr | -0.004 | 0.0006 |      -6.87 |\n",
    "| Married        |  0.063 | 0.0168 |       3.76 |\n",
    "| Union          |  0.108 | 0.0178 |       6.04 |\n",
    "R² = 0.178 <br>\n",
    "σ² = 0.124 <br>\n",
    "λ = 0.640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short introduction to Hausman test\n",
    "\n",
    "It is evident from the previous question that RE has the advantage over FE, in that time-invariant variables are not demeaned away. But if $E[c_{i}\\boldsymbol{x}_{it}] \\neq \\boldsymbol{0}$, then the RE estimator is inconsistent, where the FE estimator is consistent (but inefficient).\n",
    "\n",
    "We can use the results from the FE and RE estimations to test if RE is consistent, by calculating the following test statistics,\n",
    "\n",
    "$$\n",
    "H := (\\hat{\\mathbf{\\beta}}_{FE} - \\hat{\\mathbf{\\beta}}_{RE})'[\\widehat{\\mathrm{avar}}(\\hat{\\mathbf{\\beta}}_{FE}) - \\widehat{\\mathrm{avar}}(\\hat{\\mathbf{\\beta}}_{RE})]^{-1}(\\hat{\\mathbf{\\beta}}_{FE}-\\hat{\\mathbf{\\beta}}_{RE})\\overset{d}{\\to}\\chi_{M}^{2}, \\tag{7}\n",
    "$$\n",
    "\n",
    "*Note* The vector for $\\hat{\\mathbf{\\beta}}_{RE}$ is excluded the time invariant variables, as these are not present in $\\hat{\\mathbf{\\beta}}_{FE}$. <br>\n",
    "*Note2:* $\\widehat{\\mathrm{avar}}(\\hat{\\mathbf{\\beta}}_{RE})$ means the RE covariance (but again, we only keep the rows and columns that are for the time-variant variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Comparing FE and RE\n",
    "Use the results from the FE and RE esimtations to calculate eq. (7).\n",
    "\n",
    "* Start by calculating the differences in the FE and RE coefficients $\\hat{\\mathbf{\\beta}}_{FE} - \\hat{\\mathbf{\\beta}}_{RE}$ (again, remember to remove the time invariant columns from RE)\n",
    "* Then calculate the differences in the covariances $\\widehat{\\mathrm{avar}}(\\hat{\\mathbf{\\beta}}_{FE}) - \\widehat{\\mathrm{avar}}(\\hat{\\mathbf{\\beta}}_{RE})$ (you need to keep the \"lower right\" part of the RE covariance)\n",
    "* You now have the parts to calculate the H statistics in eq. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "# Follow the steps in the question\n",
    "\n",
    "b_re = re_result['b_hat']\n",
    "b_re = b_re[4:]\n",
    "\n",
    "hat_diff = fe_result['b_hat'] - b_re  # The differences in beta hat\n",
    "\n",
    "cov_re = re_result['cov']\n",
    "cov_re = cov_re[4:,4:]\n",
    "\n",
    "\n",
    "cov_diff = fe_result['cov'] - cov_re  # The difference in covariances\n",
    "H = hat_diff.T@la.inv(cov_diff)@hat_diff  # The Hausman test value\n",
    "\n",
    "# This calculates the p-value of the Hausman test.\n",
    "p_val = chi2.sf(H.item(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the covar matrices.\n",
    "# Remember to remove the FE time invarant regressors from RE\n",
    "hat_diff = fe_result['b_hat'] - re_result['b_hat'][4:]\n",
    "cov_diff = la.inv(fe_result['cov'] - re_result['cov'][4:, 4:])\n",
    "H = hat_diff.T@(cov_diff@hat_diff)\n",
    "# This takes the chi2 value, and then DF.\n",
    "p_val = chi2.sf(H.item(), hat_diff.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   b_fe     b_re    b_diff\n",
      "-------  -------  --------\n",
      " 0.1168   0.1121    0.0047\n",
      "-0.0043  -0.0041   -0.0002\n",
      " 0.0453   0.0628   -0.0175\n",
      " 0.0821   0.1074   -0.0253\n",
      "The Hausman test statistic is: 31.45, with p-value: 0.00.\n"
     ]
    }
   ],
   "source": [
    "# This code takes the results that you have made, and prints a nice looking table.\n",
    "def print_h_test(fe_result, re_result, hat_diff, p_val):\n",
    "    table = []\n",
    "    for i in range(len(hat_diff)):\n",
    "        row = [\n",
    "            fe_result['b_hat'][i], re_result['b_hat'][4:][i], hat_diff[i]\n",
    "        ]\n",
    "        table.append(row)\n",
    "\n",
    "    print(tabulate(\n",
    "        table, headers=['b_fe', 'b_re', 'b_diff'], floatfmt='.4f'\n",
    "        ))\n",
    "    print(f'The Hausman test statistic is: {H.item():.2f}, with p-value: {p_val:.2f}.')\n",
    "print_h_test(fe_result, re_result, hat_diff, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your table should look like this:\n",
    "\n",
    "| b_fe    |  b_re    | b_diff |\n",
    "| ------- |  ------- |  -------- |\n",
    " | 0.1168  |  0.1121  |   0.0048 |\n",
    "| -0.0043 |  -0.0041 |   -0.0002 |\n",
    " | 0.0453  |  0.0630  |  -0.0177 |\n",
    " | 0.0821  |  0.1077  |  -0.0256 |\n",
    "\n",
    " The Hausman test statistic is: 33.63, with p-value: 0.00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you have time: Comparing FE and FD and exogeneity test.\n",
    "\n",
    "There are some important tests that you need to know, which we outline here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Comparing FE and FD\n",
    "\n",
    "One of the assumptions we make when first-differencing is that the errors $e_{it} = \\Delta u_{it}$ should be serially uncorrelated. Where for fixed-effects it should be -0.5.\n",
    "\n",
    "We can easily test this assumption given the OLS residuals from equation (5). Run the regression (note that you will loose data for\n",
    "the *two* first periods)\n",
    "\\begin{equation}\n",
    "\\hat{e}_{it}=\\rho\\hat{e}_{it-1}+error_{it},\\quad t=\\color{red}{3},\\dotsc,T,\\quad i=1,\\dotsc,N\n",
    "\\end{equation}\n",
    "\n",
    "Do you find any evidence for serial correlation? Which of FD or FE seems most appropriate.\n",
    "\n",
    "*Note:* To lag an array, use the following permutation matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_ T :=\n",
    "\\begin{equation}\n",
    "\\underset{T - 1\\times T}{\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 & \\cdots & 0 & 0\\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & \\cdots & 1 & 0\n",
    "\\end{pmatrix}\n",
    "}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "*Note2:* You should end up with two columns: one with the residuals and one with the lagged residuals. To make these the same length, you need to remove the first observation for each person in the non-lagged residuals. You can use the `year` variable to do a boolean indexing of the numpy array. (But you need to first remove the very first year, since this was already removed when doing first differencing. But this is done for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "# You should have stored the results from the first differencing, use the beta hat to calculate the residuals e.\n",
    "# Then create the lag matrix P, and use the perm function with P to lag the residual e.\n",
    "# Remove the first observation for each person in the non-lagged e.\n",
    "# Estimate by ols e on its lag.\n",
    "# Print out in a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_year = year[year != 1980]\n",
    "def serial_corr(y, x, t, year):\n",
    "    b_hat = lm.est_ols(y, x)\n",
    "    e = y - x@b_hat\n",
    "    \n",
    "    # Create a lag to estimate the error on.\n",
    "    L_T = np.eye(t, k=-1)\n",
    "    L_T = L_T[1:]\n",
    "    \n",
    "    \n",
    "    e_l = lm.perm(L_T, e)\n",
    "    \n",
    "    \n",
    "    print(e.shape)\n",
    "    # We then need to remove the first obs for every person again.\n",
    "    e = e[year != 1981]\n",
    "    print(e.shape)\n",
    "\n",
    "    return lm.estimate(e, e_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3815, 1)\n",
      "(3270, 1)\n",
      "Serial Correlation\n",
      "Dependent variable: OLS residual, eᵢₜ\n",
      "\n",
      "          Beta      Se    t-values\n",
      "-----  -------  ------  ----------\n",
      "eᵢₜ₋₁  -0.3961  0.0147    -27.0185\n",
      "R² = 0.182\n",
      "σ² = 0.143\n"
     ]
    }
   ],
   "source": [
    "corr_result = serial_corr(y_diff, x_diff, t-1, reduced_year)\n",
    "\n",
    "label_ye = 'OLS residual, e\\u1d62\\u209c'\n",
    "label_e = ['e\\u1d62\\u209c\\u208B\\u2081']\n",
    "title = 'Serial Correlation'\n",
    "lm.print_table(\n",
    "    (label_ye, label_e), corr_result, \n",
    "    title='Serial Correlation', floatfmt='.4f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a table that look like this:\n",
    "\n",
    "Serial Correlation <br>\n",
    "Dependent variable: OLS residual, eᵢₜ\n",
    "\n",
    "|       |    Beta |     Se |   t-values |\n",
    "|-------|---------|--------|------------|\n",
    "| eᵢₜ₋₁ | -0.3961 | 0.0147 |   -27.0185 |\n",
    "R² = 0.182 <br>\n",
    "σ² = 0.143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Test for strict exogeneity\n",
    "\n",
    "Add a lead of the union variable, $unioni_{t+1}$ to the equation (3) (note that you will lose data from period $T$ , 1987) and estimate the model with *fixed effects* (i.e., you have to demean $unioni_{t+1}$ along with all the other variables and throw out time constant variables). Is $unioni_{t+1}$ significant? What does this imply for the strict exogeneity assumption?\n",
    "\n",
    "*Hint:* To lead a variable, think along the same lines as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "# Create first a lead matrix, and use this to transform the union variable only. \n",
    "# Remove the last observation for each person using the year column to do an boolean index. Then you can hstack the lead union variable to you x-matrix. \n",
    "# Now within transform the data, and estimate using FE as you have done before. \n",
    "# Is the lead variable significant/insignificant? What does that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exogeneity test\n",
      "Dependent variable: Log wage\n",
      "\n",
      "                   Beta      Se    t-values\n",
      "--------------  -------  ------  ----------\n",
      "Experience       0.1213  0.0100     12.1001\n",
      "Experience sqr  -0.0050  0.0008     -6.3579\n",
      "Married          0.0436  0.0209      2.0898\n",
      "Union            0.0757  0.0218      3.4784\n",
      "Union lead       0.0515  0.0223      2.3063\n",
      "R² = 0.146\n",
      "σ² = 0.128\n"
     ]
    }
   ],
   "source": [
    "def exogeneity_test(x, y, t, year):\n",
    "    # Create lead\n",
    "    F_T = np.eye(t, k=1)\n",
    "    F_T = F_T[:-1]\n",
    "\n",
    "    # Lead union\n",
    "    union_lead = lm.perm(F_T, x[:, 7].reshape(-1, 1))\n",
    "\n",
    "    # Collect variables to test for exogeneity\n",
    "    x_exo = x[year != 1987]\n",
    "    x_exo = np.hstack((x_exo, union_lead))\n",
    "    y_exo = y[year != 1987]\n",
    "\n",
    "    # Within transform the data\n",
    "    Q_T = demeaning_matrix(t - 1)\n",
    "\n",
    "    yw_exo = lm.perm(Q_T, y_exo)\n",
    "    xw_exo = lm.perm(Q_T, x_exo)\n",
    "    xw_exo = xw_exo[:, 4:]\n",
    "\n",
    "    label_exo = label_x_fe + ['Union lead']\n",
    "    n = y.size/t\n",
    "    # Estimate model\n",
    "    exo_test = lm.estimate(\n",
    "        yw_exo, xw_exo, t=t - 1, transform='fe'\n",
    "    )\n",
    "\n",
    "    lm.print_table(\n",
    "        (label_y, label_exo), \n",
    "        exo_test, title='Exogeneity test', floatfmt='.4f'\n",
    "    )\n",
    "exogeneity_test(x, y, t, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table should look something like this:\n",
    "Exogeneity test <br>\n",
    "Dependent variable: Log wage\n",
    "\n",
    "|                |    Beta |     Se |   t-values |\n",
    "|----------------|---------|--------|------------|\n",
    "| Experience     |  0.1213 | 0.0100 |    12.1001 |\n",
    "| Experience sqr | -0.0050 | 0.0008 |    -6.3579 |\n",
    "| Married        |  0.0436 | 0.0209 |     2.0898 |\n",
    "| Union          |  0.0757 | 0.0218 |     3.4784 |\n",
    "| Union lead     |  0.0515 | 0.0223 |     2.3063 |\n",
    "R² = 0.146<br>\n",
    "σ² = 0.128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
