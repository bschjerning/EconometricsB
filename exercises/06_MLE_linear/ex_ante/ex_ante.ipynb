{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general modules \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "# Code for this week \n",
    "import estimation_ante as est\n",
    "import LinearModel as lm # based on code from previous weeks\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "plt.rcParams.update({\n",
    "    #\"text.usetex\": True, # LaTeX can sometimes be tricky to get working but makes graphs prettier :) \n",
    "    \"font.family\": \"serif\", \n",
    "    \"font.size\":18 \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation of the Linear Model\n",
    "\n",
    "\n",
    "In this exercise we will consider the linear regression model. Of\n",
    "course, when estimated with OLS, the estimator, which minimizes the sum\n",
    "of squared residuals, has a closed form solution. This also goes for the\n",
    "maximum likelihood estimator when the residuals are assumed Gaussian. We\n",
    "will however do the maximization numerically using the scipy library and the `optimize` class, more specifically we will use its `minimize` function. The purpose of this exercise is to learn to do\n",
    "numerical maximization and to be familiar with $M$-estimators by viewing\n",
    "the maximum likelihood estimator of the linear model as an\n",
    "$M$-estimator.\n",
    "\n",
    "## The Model\n",
    "\n",
    "We consider a linear model with the following characteristics\n",
    "\n",
    "$$y_{i}=\\beta _{0}+\\beta _{1}x_{1i}+....+\\beta _{k-1}x_{k-1i}+u _{i} \\quad i=1,..,N$$\n",
    "\n",
    "with $u{i} \\sim N(0,\\sigma_{\\varepsilon}^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (conditional) likelihood contribution for observation $i$ is,\n",
    "$$f\\left(y_{i}\\left|\\mathbf{x}_{i};\\beta,\\sigma^{2}\\right.\\right) = \\phi\\left(\\frac{\\hat{u}_i}{\\sigma}\\right) =\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{1}{2}\\frac{\\hat{u}_{i}^{2}}{\\sigma^{2}}\\right\\},$$\n",
    "where\n",
    "$\\hat{u}_{i}=y_{i}- \\sum_{k=1}^K \\beta_k x_{ik}$, and $\\phi(\\cdot)$ is the standard normal density. \n",
    "\n",
    "Thus, the loglikelihood contribution is \n",
    "$$\n",
    "\\ell_i(\\theta) = - \\frac{1}{2}\\log (2 \\pi) - \\frac{1}{2}\\log(\\sigma^2) - \\frac{1}{2}\\frac{\\hat{u}_i^2}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Often the term $-\\frac{N}{2}\\log\\left(2\\pi\\right)$ is dropped as it does not change with \n",
    "$\\beta$ or $\\sigma$ and thus does not affect the optimization. \n",
    "\n",
    "Finally, our optimizer will be solving the problem \n",
    "$$ \\min_\\theta N^{-1} \\sum_{i=1}^N q(\\theta, y_i, x_i),$$ \n",
    "where the criterion function is the negative loglikelihood, $q(\\theta,y_i,x_i) = -\\ell_i(\\theta)$. \n",
    "Again, we can drop the factor $N^{-1}$ as it does not affect the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate dataset\n",
    "n = 100\n",
    "K = 2 # two regressors, a constant and one (real) regressor\n",
    "beta = np.ones((K,1))  # First is constant\n",
    "sigma = 3\n",
    "true_theta = np.vstack([beta, sigma])\n",
    "y, x = lm.sim_data(n, true_theta, rng)\n",
    "\n",
    "# Find some starting values \n",
    "theta0 = lm.starting_values(y, x, )\n",
    "theta0 = 0.8*theta0 # scale them by 0.8 to make the problem a little harder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Write a function for the likelihood contribution.\n",
    "Open the file `LinearModel.py`, and fill in the function `loglikelihood` with the **likelihood contribution**. It should return an $N \\times 1$ vector of likelihood contributions.\n",
    "\n",
    "*Hint:* The sum of the likelihood contributions should be close to -162.8, given `theta0, x, y`, which I have written a function to check for you, so you know if you have written it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts of the lm.loglikelihood() function.\n",
    "# First, calculate the residual.\n",
    "# Then calculate the likelihood value, using the likelihood contribution equation from above.\n",
    "# Test if you got it right with the cell below. You might have to \"reseed\" by running the first cell in this notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(np.sum(lm.loglikelihood(theta0, y, x)), -162.800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Estimate Parameters \n",
    "\n",
    "Now finish up the `estimate` function, which takes an function to minimize `func`, starting values `theta0`, the data `y` and `x`, and what type of variance to use `cov_type`.\n",
    "\n",
    "You need to use the `minimize` function, which takes the following inputs: the objective function `obj_func`, and the starting values `theta0`.\n",
    "\n",
    "You also need to finish up the `variance` function, which takes the function `func` (not the objective function), the data `y` and `x`, the results from the minimizer `result`, and finaly what type of variance to calculate `cov_type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Estimate Parameters with `optimize.minimize`\n",
    "\n",
    "1. Create a `lambda` function, `Q`, taking only one input, `theta`, and returning the negative mean loglikelihood. \n",
    "    * ***Hint:*** Watch this video to learn about functions in Python: https://youtu.be/watch?v=loF8zsPaIjs. \n",
    "2. Evaluate `Q(theta0)` to test that it works. \n",
    "3. Call `minimize`, starting from `theta0`, with options having `disp` set to `True`, and the optimization algorithm (`method`) set to `BFGS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice example with optimize.minimize\n",
    "\n",
    "# 1. function handle to the objective \n",
    "Q = lambda theta : np.mean(lm.q(theta, y, x)) # just a function of one variable\n",
    "\n",
    "# 2. starting values \n",
    "theta0 = lm.starting_values()\n",
    "\n",
    "# 3. call scipy minimize \n",
    "#res = minimize(FILL IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Standard Errors\n",
    "\n",
    "**Tasks**: \n",
    "* Fill in `estimation.estimate()`: For the estimate function, you need to read the documentation for the optimize.minimize function, to pass the obj_func and theta0 to that function.\n",
    "    * ***Bonus:*** Make sure that `estimate()` passes the inputs `options` and the optional `kwargs` correctly to `optimize.minimize`. The options struct can e.g. ask the optimizer to print or not print final convergence output by setting `options = {'disp': True}` (or `False`). The `kwargs` can include things like controlling which algorithm is used for optimization, e.g. `method='BFGS'`. \n",
    "* Fill in `estimation.variance()`: implement all three options for $\\text{Avar}(\\hat{\\theta}$) described below. \n",
    "\n",
    "## Theory: The Three Asymptotic Variance Estimators\n",
    "\n",
    "The log-likelihood function is a nonlinear function, which must in\n",
    "general be maximized numerically in order to obtain the ML estimates.\n",
    "In general, for $M$-estimators, we know that \n",
    "$$\n",
    "\\sqrt{N}\\left( \\boldsymbol{\\hat{\\theta}}-\\boldsymbol{\\theta }_{0}\\right) \n",
    "\\overset{d}{\\rightarrow} \\mathcal{N} \\left(\\mathbf{0}, \\mathbf{A}_{0}^{-1} \\mathbf{B}_0 \\mathbf{A}_{0}^{-1} \\right). \n",
    "$$ \n",
    "\n",
    "For Maximum Likelihood (ML) estimators specifically, the *Information Matrix \n",
    "Equality* holds, which implies that \n",
    "$$ \\mathbf{A}_{0} = \\mathbf{B}_{0}. $$ \n",
    "This means that the asymptotic variance matrix simplifies so that \n",
    "$$ \\mathbf{A}_{0}^{-1} \\mathbf{B}_0 \\mathbf{A}_{0}^{-1} = \\mathbf{A}_{0}^{-1} = \\mathbf{B}_{0}^{-1}.$$ \n",
    "\n",
    "This means that we have three valid ways of estimating the asymptotic variance \n",
    "matrix of our parameter esimates: \n",
    "\n",
    "$\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}})$ can be taken to be any\n",
    "of the three options\n",
    "1. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{A}}^{-1}$: the `Hessian`, \n",
    "2. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{B}}$: the `Outer Product`,  \n",
    "3. $\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = N^{-1} \\hat{\\mathbf{A}}^{-1} \\hat{\\mathbf{B}} \\hat{\\mathbf{A}}^{-1} $: the `Sandwich`: viewed as a more \"robust\" option.  \n",
    "\n",
    "\n",
    "where the components matrices are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\quad \\mathbf{\\hat{A}} \n",
    "    &= -\\frac{1}{N} \\left[ \\sum_{i=1}^{N}\\mathbf{H}_{i}( \\boldsymbol{\\hat{\\theta}}) \\right] \n",
    "\\\\\n",
    "\\quad  \\mathbf{\\hat{B}}\n",
    "    &= \\frac{1}{N}\n",
    "        \\left[ \\sum_{i=1}^{N} \\mathbf{s}_{i}( \\boldsymbol{\\hat{\\theta}}) \n",
    "                              \\mathbf{s}_{i}( \\boldsymbol{\\hat{\\theta}})^{\\prime}\n",
    "        \\right].\n",
    "\\end{aligned}\n",
    "$$ \n",
    "\n",
    "\n",
    "*Programming hint:* To calculate the variance you have to do the following:\n",
    "\n",
    "* `Hessian`: The optimizer will return the inverse of the Hessian, and can be accessed through `result.hess_inv`, which is $\\mathbf{\\hat{A}}^{-1}$. So for the `Hessian` variance, you would calculate:\n",
    "$$\n",
    "\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = \\frac{1}{N}\\mathbf{\\hat{A}}^{-1}\n",
    "$$\n",
    "    * *Note:* You do not have to divide `result.hess_inv` by $N$, since we give the *average* loglikelihood to `minimize`.\n",
    "* `Outer Product`: the ex ante code computes the *numerical gradient* $N \\times K$ matrix, `s`. You need to compute the $K \\times K$ outer product of the scores, \n",
    "$ \\mathbf{s}' \\mathbf{s} = \\sum_{i=1}^N \\mathbf{s}_i \\mathbf{s}_i', $\n",
    "($\\mathbf{s}_i$ is $K \\times 1$ in this notation) and then use this to form $\\hat{\\mathbf{B}} = N^{-1} \\mathbf{s}' \\mathbf{s}$. Finally, calculate the variance using:\n",
    "$$\n",
    "\\widehat{\\text{Avar}}( \\boldsymbol{\\hat{\\theta}}) = \\frac{1}{N}\\mathbf{\\hat{B}}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.a: The Outer Product\n",
    "\n",
    "Estimate parameters and compute standard errors using the `Outer Product` estimator. (This is the *default* variance estimator in `est.estimate`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = est.estimate(lm.q, theta0, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['beta 1', 'beta 2', 'sigma2']\n",
    "est.print_table(label, results1, title='Maximum Likelihood results', num_decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your table should look a little like this: <br>\n",
    "\n",
    "Maximum Likelihood results <br>\n",
    "\n",
    "|        |   theta |   se |   t |\n",
    "|--------|--------|------|------------|\n",
    "| beta 1 |   0.99 | 0.31 |       3.23 |\n",
    "| beta 2 |   1.36 | 0.40 |       3.42 |\n",
    "| sigma2 |   2.91 | 0.23 |      12.66 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.b: The 'Sandwich' Estimator\n",
    "\n",
    "Compute standard errors using the `Sandwich` estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_san = est.estimate(lm.q, theta0, y, x, cov_type='Sandwich')\n",
    "est.print_table(label, results_san, title='Maximum Likelihood results', num_decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should look like this: \n",
    "\n",
    "|        |   theta |    se |      t |\n",
    "|:-------|--------:|------:|-------:|\n",
    "| beta 1 |   0.986 | 0.284 |  3.475 |\n",
    "| beta 2 |   1.358 | 0.336 |  4.044 |\n",
    "| sigma2 |   2.911 | 0.186 | 15.619 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.c: The 'Hessian' Estimator\n",
    "\n",
    "Compute standard errors using the `Hessian` estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_he = est.estimate(lm.q, theta0, y, x, cov_type='Hessian')\n",
    "est.print_table(label, results_he, title='Maximum Likelihood results', num_decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results should look like this: \n",
    "\n",
    "|        |   theta |    se |      t |\n",
    "|:-------|--------:|------:|-------:|\n",
    "| beta 1 |   0.986 | 0.284 |  3.468 |\n",
    "| beta 2 |   1.358 | 0.362 |  3.755 |\n",
    "| sigma2 |   2.911 | 0.200 | 14.586 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Monte Carlo Study. \n",
    "\n",
    "**Task:** Conduct a Monte Carlo study for different sample sizes. \n",
    "\n",
    "Conduct a Monte Carlo study of the Maximum Likelihood estimator. Try\n",
    "various values of $N$ to illustrate the consistency of the\n",
    "estimator. Is the estimator biased? Compare the three types of\n",
    "standard error estimates to the Monte Carlo sampling standard\n",
    "deviation. Is the estimator of $\\sigma$ consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = [5, 15, 50, 200]  # Sample size\n",
    "S = 1_000  # Number of replications\n",
    "P = len(theta0)\n",
    "\n",
    "# Initialize containers for all MX experiments\n",
    "theta_n    = np.zeros((len(NN), P, S))\n",
    "se_theta_n = np.zeros((len(NN), P, S))\n",
    "MC_se      = np.zeros((len(NN), P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, N in enumerate(NN): # loop over sample sizes \n",
    "    print(f'N = {N:5d}: {i+1}/{len(NN)}')\n",
    "    for s in range(S): # for each Monte Carlo replication \n",
    "        y, x = # simulate N observations \n",
    "        theta0 = # find starting values, and scale them slightly by 0.8\n",
    "        results = # estimate parameters \n",
    "        \n",
    "        theta_n[i, :, s] = results['theta_hat']\n",
    "        se_theta_n[i, :, s] = results['se']\n",
    "    \n",
    "    MC_se[i, :] = np.std(theta_n[i,:,:], axis=1, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel() \n",
    "i_theta = 1 # second beta, i.e. not the constant \n",
    "theta_diff = theta_n[:, i_theta, :] - true_theta[i_theta, 0]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(theta_diff[i,:], bins=20)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_xlabel('$\\\\hat{\\\\theta}_1 - \\\\theta^o_1$')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "aa = np.linspace(0.3,0.5,len(NN))\n",
    "for i, N in enumerate(NN): \n",
    "    xx = np.linspace(-6,6,30)\n",
    "    ax.hist(theta_diff[i, :], bins=xx, alpha=aa[i], label=f'N = {N}', density=True)\n",
    "ax.legend(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same graph, scaled by $\\sqrt{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel() \n",
    "i_theta = 1 # second beta, i.e. not the constant \n",
    "theta_diff = theta_n[:, i_theta, :] - true_theta[i_theta, 0]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.hist(theta_diff[i,:] * np.sqrt(NN[i]), bins=30)\n",
    "    ax.set_xlim(-15, 15)\n",
    "    \n",
    "    ax.set_xlabel(f'$\\sqrt{{N}} (\\\\hat{{\\\\theta}}_{i_theta} - \\\\theta^o_{i_theta})$')\n",
    "    ax.set_ylabel('Monte carlo samples')\n",
    "    ax.set_title(f'N = {NN[i]}')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Alternative Minimization Algorithms\n",
    "\n",
    "**Task:** Estimate the model using alternative minimization algorithms (the input `method` to `minimize`). Compare how many function evaluations they take and whether they converge to the global minimum. \n",
    "\n",
    "* `BFGS`: The default algorithm (Newton with approximated Hessian and numerical gradients), \n",
    "* `CG`: Newton with numerical Hessian and gradients,  \n",
    "* `Nelder-Mead`: Gradient-free optimizer, \n",
    "* `Powell`: Another gradient-free optimizer. \n",
    "\n",
    "***Hint:*** `est.estimate()` accepts various extra args, which are by default just passed to `scipy.optimize.minimize`. Try alternatives for `method` (the algorithm). \n",
    "\n",
    "***Note:*** The gradient-free optimizers do not return an inverse Hessian, so we can only compute the Outer Product variance matrix for those (unless we compute the Hessian numerically). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "y, x = lm.sim_data(N, true_theta, rng)\n",
    "theta0 = lm.starting_values(y, x)*0.8\n",
    "\n",
    "results_BFGS = est.estimate(lm.q, theta0, y, x) # the default option is method='BFGS'\n",
    "results_CG   = # estimate using method='CG'\n",
    "results_NM   = # and with method='Nelder-Mead'\n",
    "results_PO   = # and with method='Powell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est.print_table(label, results_BFGS))\n",
    "print(est.print_table(label, results_CG))\n",
    "print(est.print_table(label, results_NM))\n",
    "print(est.print_table(label, results_PO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BFGS ---\n",
    "# Optimizer succeeded after 14 iter. (60 func. evals.). Final criterion:    1.533.\n",
    "#Results\n",
    "#         theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2351\n",
    "# beta 2  0.4773  0.2920   1.6345\n",
    "# sigma2  2.8084  0.1992  14.0965\n",
    "# --- CG ---\n",
    "# Optimizer succeeded after 8 iter. (76 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2354\n",
    "# beta 2  0.4774  0.2920   1.6345\n",
    "# sigma2  2.8084  0.1992  14.0964\n",
    "# --- Nelder-Mead ---\n",
    "# Optimizer succeeded after 64 iter. (116 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6397  0.2862   2.2351\n",
    "# beta 2  0.4773  0.2921   1.6343\n",
    "# sigma2  2.8084  0.1992  14.0963\n",
    "# --- Powell ---\n",
    "# Optimizer succeeded after 2 iter. (69 func. evals.). Final criterion:    1.533.\n",
    "# Results\n",
    "#          theta      se        t\n",
    "# beta 1  0.6396  0.2862   2.2348\n",
    "# beta 2  0.4773  0.2921   1.6342\n",
    "# sigma2  2.8084  0.1992  14.0963"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
