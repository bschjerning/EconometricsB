{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Problem set 8 : Binary response models for panel data: Monte Carlo Evidence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A dynamic binary response model for panel data\n",
    "The overall purpose of this exercise is to use Monte Carlo simulation to investigate the properties of estimators for binary response models for panel data. \n",
    "\n",
    "For concreteness, consider the dynamic panel data model specified below\n",
    "\n",
    "\\begin{align} \n",
    "y_{it}&=&\\mathbb{1}(\\delta z_{it}  + \\rho y_{it-1} + c_i +e_{it}>0) \\\\\n",
    "y_{i0}&=&\\mathbb{1}(\\eta_i>0) \\\\\n",
    "c_i   &=& \\phi_0 + \\phi_{y0} y_{i0} + a_i\\\\\n",
    "a_i &\\sim&  iidN(0, \\sigma_a^2)\\\\\n",
    "e_{it} &\\sim& iidN(0, 1) \\\\\n",
    "\\eta_{i} &\\sim& iidN(0, 1) \n",
    "\\end{align}\n",
    "\n",
    "where the scalar random variable $z_{it}$ is assumed to be iid standard normally distributed. Depending on the parameters, this model can be either dynamic (if $\\rho \\ne 0$) or static (if $\\rho = 0$) and may or may not contain unobserved effects depending on the parameters that determine $c_i$. \n",
    "\n",
    "We are specifically interested in estimating the effect of $z_{it}$ and $y_{it-1}$ on $P(y_{it}=1|z_{it}, y_{it-1}, c_i)$, that is the effect of changing $z_{it}$ or $y_{it-1}$ holding constant other variables including $c_i$. \n",
    "\n",
    "## 2. Research questions\n",
    "During the lectures on \"Binary response models for panel data\" we considered a battery of different models and estimators. Each method may or may not be appropriate depending on the nature of the data and what the object of interest is. Sometimes we are satisfied with estimating the direction of an effect that may be determined by the sign of the coefficient to single explanatory variable. Sometimes the interest is in the average partial effect (APE) and other times we need to make inference about the whole distribution of choice probabilities and corresponding partial effects. \n",
    "\n",
    "Each estimator are derived under different assumptions, and will be appropriate in different context and have different asymptotic properties. This may for example depend on \n",
    "1. whether there are **unobserved effects** in the data and whether they are **correlated with explanatory variables**\n",
    "1. whether the model is **static** or **dynamic**, for example because it contains a lagged dependent variable \n",
    "1. whether you consider a **pooled analysis** or specify a model for **the conditional distribution for the entire sequence** of observed binary outcomes \n",
    "1. how modest your ambitions are in terms of the **object of interest**\n",
    "\n",
    "This motivates a menu research questions: \n",
    "1. Is it possible to identify the parameter of interest? \n",
    "1. What is the appropriate estimator? \n",
    "1. Is the estimator $\\sqrt{N}$-consistent and asymptotic normal? \n",
    "1. Is it unbiased?\n",
    "1. Is it efficient?\n",
    "1. Are asymptotic results a good approximation for the sample size under consideration? \n",
    "1. Are the usual standard errors and tests-statistics valid for inference, or do I need to compute robust standard errors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models to compare\n",
    "To analyze the questions above, you are asked to perform a series of Monte Carlo experiments where you change the sampling scheme using the model specified above and compare the performance of different estimators derived from different models such as: \n",
    "\n",
    "1. **Linear probability models with unobserved effects** estimated using least squares methods such as for example Pooled OLS, Fixed Effects, First Differencing IV methods etc. \n",
    "1. **Pooled index models** estimated using partial MLE, such as Pooled Probit or Pooled Logit \n",
    "1. **Index models with unobserved effects under strict exogeneity** such as the Random effects Probit, Chamberlain's Correlated Random effects Probit or Fixed Effects Logit. \n",
    "1. **Dynamic unobserved effects models** such as the Dynamic version of the Correlated Random effects Probit \n",
    "\n",
    "As a testbed for different estimators we use the dynamic binary probit model with unobserved effects described in Section 1. Use the model to generate data sets for different values of parameters and sample size, and investigate the properties of different estimators. \n",
    "\n",
    "**Your analysis does not have to be exhaustive**, but you need to consider *at least one method from at least three out of the four classes mentioned above*. Divide your analysis into static models and dynamic models. In the latter case, special focus in on the causal effect of the lagged dependent variable (state dependence). Here you need to discriminate between true and *spurious state dependence* and analyze the importance of accounting for unobserved heterogeneity and the initial conditions problem. \n",
    "\n",
    "The same rule applies regarding your analysis of the properties of the estimators you apply. Think of the list of research questions above as a menu of opportunities. In the next section I give more examples of specific questions to analyze. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Some advice on Monte Carlo: Analyzing properties of estimators\n",
    "I recommend you start simple. Generate data from a model where you know you should be able to recover the underlying parameters by estimating a well specified model that is consistent with how you generated the data. Then you can set up more sophisticated experiments such as the ones we discussed during lectures: \n",
    "\n",
    "#### Example experiments - static models\n",
    "1. No heterogeneity \n",
    "    - does LPM give a good approximation of APE?\n",
    "    - does pooled probit estimate true parameters?\n",
    "    \n",
    "1. Neglected heterogeneity - Pooled probit or LPM\n",
    "   - does pooled probit estimate true parameters?\n",
    "   - does pooled OLS and probit still estimate APE?\n",
    "\n",
    "1. Can RE-Probit estimate account for heterogeneity and uncover true parameters\n",
    "\n",
    "#### Example experiments - dynamic models   \n",
    "1. Neglecting heterogeneity and initial conditions\n",
    "   - does pooled OLS and probit consistently estimate APE of lagged y (state dependence)?\n",
    "   - what about other parameters?\n",
    "1. Accounting for heterogeneity and initial conditions \n",
    "    - is LPM-FE valid for dynamic models?\n",
    "    - can RE probit estimate APE of lagged y (state dependence)?\n",
    "    - what if explanatory variables are correlated with c_i?\n",
    "\n",
    "In the simulation exercise in the lectures on binary response for panel data, we essentially did Monte Carlo using only a single Monte Carlo sample. To appropriately analyze the distribution of an estimator, test-statistic, etc. you need to generate many samples by repeatedly simulating data and estimating the parameter of interest. Using this information, we can for example learn about the distribution of an estimator. As an example, the notebook [clogit_simulations.ipynb](https://github.com/bschjerning/EconometricsB/blob/main/lectures/14_multinomial_response/clogit_simulations.ipynb) provides a simple example in the context of the Condtional Logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 More resources\n",
    "The most relevant sections is Wooldridge's textbook are (in order of priority)\n",
    "- Section 15.8 on Binary Response models for Panel data\n",
    "- Section 12.8.1 on Monte Carlo Simulation \n",
    "- Section 13.8 and 13.9 on Partial/Pooled MLE and likelihood based panel data methods with unobserved effects. \n",
    "- Sections 12.1-12.3, and 12.5.1 on the properties of M-Estimators\n",
    "\n",
    "Most of of the relevant estimation methods and econometric models are reviewed in the lectures 12 and 13 on \"Binary response models for panel data\" and implemented in the notebook [binary_choice_panel.ipynb](https://github.com/bschjerning/EconometricsB/blob/main/lectures/12_binary_response_panel/binary_choice_panel.ipynb). Here you will also find demonstrations of the code that implements a selection of panel data methods for binary response. For convenience, I located the current notebook in the same directory [12_binary_response_panel](https://github.com/bschjerning/EconometricsB/tree/main/lectures/12_binary_response_panel), so that you can easily access all resources. \n",
    "\n",
    "For Monte Carlo, you may get some inspiration from the first exercise set, where we do a simple Monte Carlo Experiment for the linear model. You may also want to look a the last lecture on the simulation and maximum likelihood estimation of the conditional logit model. As mentioned above, The notebook [clogit_simulations.ipynb](https://github.com/bschjerning/EconometricsB/blob/main/lectures/14_multinomial_response/clogit_simulations.ipynb) briefly illustrates how properties the maximum likelihood estimator for conditional logit can be analyzed though a simple Monte Carlo experiment. \n",
    "\n",
    "You may also need the code used during the exercise classes, or even take the challenge of writing code for estimators I have not implemented, such as the Fixed Effects Logit model. You are encouraged to use all resources available and modify it so it fits your application. If you take up this challenge. Start simple. Consider first the model for only two periods, and move on from there.\n",
    "\n",
    "It may also be that you want to modify the sampling scheme. For example, in the model above (and associated simulation code) $z_{it}$ is assumed to be uncorrelated with $c_i$. So if your are interested in analyzing the consequences of correlated unobserved effects, you are welcome to extend the simulation framework to for example allow for correlation between unobserved effects and the explanatory variables.  \n",
    "\n",
    "## 6. Getting started\n",
    "To make sure we are all on the same page, lets do a few initial steps here \n",
    "\n",
    "### Initial setup\n",
    "Before we begin, lets read in some modules used though-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Routine for simulation of model\n",
    "The model outlined above is implemented in the function ''simulate'' in indexmodels.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>period</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>y0</th>\n",
       "      <th>const</th>\n",
       "      <th>l1.y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.271649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.343861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.571454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354558</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>999.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.154665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>999.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.093586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>999.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>999.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.066231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>999.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.743087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       group  period    y         z   y0  const  l1.y\n",
       "1        0.0     1.0  0.0 -1.271649  1.0    1.0   1.0\n",
       "2        0.0     2.0  0.0 -1.343861  1.0    1.0   0.0\n",
       "3        0.0     3.0  1.0 -0.571454  1.0    1.0   0.0\n",
       "4        0.0     4.0  1.0  0.015299  1.0    1.0   1.0\n",
       "5        0.0     5.0  1.0  0.354558  1.0    1.0   1.0\n",
       "...      ...     ...  ...       ...  ...    ...   ...\n",
       "10995  999.0     6.0  1.0 -0.154665  0.0    1.0   1.0\n",
       "10996  999.0     7.0  1.0  0.093586  0.0    1.0   1.0\n",
       "10997  999.0     8.0  1.0  0.505401  0.0    1.0   1.0\n",
       "10998  999.0     9.0  1.0 -1.066231  0.0    1.0   1.0\n",
       "10999  999.0    10.0  1.0  0.743087  0.0    1.0   1.0\n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulate data from dynamic model\n",
    "from indexmodels import *\n",
    "df_sim=simulate(n=1000, nT=10, model='probit', rho=.1, delta=1, phi_0=0.2, phi_y0=0.2, sigma_a=1)\n",
    "df_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines for estimators and estimation methods\n",
    "The file `indexmodels.py` also contain implementations of pooled probit/logit and random effects probit/logit models. It relies on `mestim.py` that contains a routine for implementation of the general class of M-estimators known from Chapter 12 - including MLE (if you specify the objective function appropriately).  \n",
    "The file `linearpaneldata.py` includes simple routines for pooled ols and fixed effects regressions. Here you may want to subplement with the code from exercise classes on static and dynamic linear panel data models.  \n",
    "\n",
    "As a starting point, we could for example use these routines to simulate data from a static unobserved effects model with one strictly exogneous explanatory variable, $z_{it}$, and estimate the effect of $z_{it}$ using a variety of methods. You may get something like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Specification: Pooled OLS Panel Regression\n",
      "Dep. var. : y \n",
      "\n",
      "parnames         b_hat          se    t-values\n",
      "----------  ----------  ----------  ----------\n",
      "const           0.5340      0.0119     44.8462\n",
      "z               0.1136      0.0040     28.0835\n",
      "# of groups:       1000\n",
      "# of observations: 20000 \n",
      "\n",
      "\n",
      "Specification: Linear Fixed Effects Regression\n",
      "Dep. var. : y \n",
      "\n",
      "parnames         b_hat          se    t-values\n",
      "----------  ----------  ----------  ----------\n",
      "z               0.1130      0.0034     33.1253\n",
      "# of groups:       1000\n",
      "# of observations: 20000 \n",
      "\n",
      "Pooled probit\n",
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               0.29791     0.01155    25.80431     0.00000     0.11348\n",
      "const           0.08889     0.03125     2.84448     0.00000     0.03386\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -13284.088933705729 \n",
      "\n",
      "Iteration info: 3 iterations, 4 evaluations of objective, and 4 evaluations of gradients\n",
      "Elapsed time: 0.1005 seconds\n",
      "\n",
      "Random effects probit\n",
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               0.70866     0.01620    43.74846     0.00001     0.11287\n",
      "const           0.18998     0.07267     2.61423    -0.00000     0.03026\n",
      "sigma_a         2.17579     0.07010    31.03940    -0.00000     0.34656\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -7147.5681691048085 \n",
      "\n",
      "Iteration info: 11 iterations, 12 evaluations of objective, and 12 evaluations of gradients\n",
      "Elapsed time: 1.1201 seconds\n",
      "\n",
      "Random effects logit\n",
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               1.25844     0.03085    40.79400     0.00000     0.07609\n",
      "const           0.34367     0.13082     2.62710     0.00000     0.02078\n",
      "sigma_a         3.90555     0.12661    30.84646    -0.00000     0.23615\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -7151.954356032121 \n",
      "\n",
      "Iteration info: 15 iterations, 16 evaluations of objective, and 16 evaluations of gradients\n",
      "Elapsed time: 0.9314 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import linearpaneldata as lpd   # simple routines to do linear FE and Pooled OLS regressions\n",
    "from indexmodels import *       # objective functions etc. for estimation of panel data binary response models\n",
    "import mestim as M              # routines for M-estimation given general sample objective functions\n",
    "\n",
    "# Simulate data\n",
    "df_sim = simulate(n=1000, nT=20, delta=0.7, rho=0.3, psi=0, phi_0=0.1,  phi_y0=0.1, sigma_a=2, \n",
    "         model='probit', rng=random.default_rng(seed=43))\n",
    "\n",
    "# Estimate models\n",
    "lpm_ols=lpd.estim(df_sim, 'y', xvar=['const','z'], groupvar='group', method='pols', cov_type='robust')\n",
    "lpm_fe=lpd.estim(df_sim, 'y',  xvar=['z'], groupvar='group', method='fe', cov_type='robust')\n",
    "res_pp=pooled(df_sim, 'y', xvar =['z', 'const'] , groupvar='group', model='probit', cov_type='sandwich')\n",
    "res_rep_prob=rand_effect(df_sim, 'y', xvar =['z', 'const'] , groupvar='group', model='probit', cov_type='Binv')\n",
    "res_rep_logit=rand_effect(df_sim, 'y', xvar =['z', 'const'] , groupvar='group', model='logit', cov_type='Binv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               0.29791     0.01155    25.80431     0.00000     0.11348\n",
      "const           0.08889     0.03125     2.84448     0.00000     0.03386\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -13284.088933705729 \n",
      "\n",
      "Iteration info: 3 iterations, 4 evaluations of objective, and 4 evaluations of gradients\n",
      "Elapsed time: 0.1172 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_output(res_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               1.25844     0.03085    40.79400     0.00000     0.07609\n",
      "const           0.34367     0.13082     2.62710     0.00000     0.02078\n",
      "sigma_a         3.90555     0.12661    30.84646    -0.00000     0.23615\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -7151.954356032121 \n",
      "\n",
      "Iteration info: 15 iterations, 16 evaluations of objective, and 16 evaluations of gradients\n",
      "Elapsed time: 0.9314 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_output(res_rep_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep. var. : y \n",
      "\n",
      "parnames      theta_hat          se    t-values         jac         APE\n",
      "----------  -----------  ----------  ----------  ----------  ----------\n",
      "z               0.70866     0.01620    43.74846     0.00001     0.11287\n",
      "const           0.18998     0.07267     2.61423    -0.00000     0.03026\n",
      "sigma_a         2.17579     0.07010    31.03940    -0.00000     0.34656\n",
      "\n",
      "# of groups:      : 1000\n",
      "# of observations : 20000\n",
      "# log-likelihood. : -7147.5681691048085 \n",
      "\n",
      "Iteration info: 11 iterations, 12 evaluations of objective, and 12 evaluations of gradients\n",
      "Elapsed time: 1.0432 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_output(res_rep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.67817832,  1.        ],\n",
       "       [ 1.        , -0.58552938,  1.        ],\n",
       "       [ 1.        , -0.90867312,  0.        ],\n",
       "       ...,\n",
       "       [ 1.        ,  1.07829422,  1.        ],\n",
       "       [ 1.        ,  0.75045829,  1.        ],\n",
       "       [ 1.        ,  2.05202139,  1.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## From df to values\n",
    "\n",
    "y = df_sim['y'].values\n",
    "t = int(max(df_sim['period'].values))\n",
    "n = int(max(df_sim['group'].values) + 1)\n",
    "T = np.tile(t, n)\n",
    "cons = np.ones(n*t).reshape(-1,1)\n",
    "z = df_sim['z'].values.reshape(-1,1)\n",
    "y_lag = df_sim['l1.y'].values.reshape(-1,1)\n",
    "x = np.column_stack((cons, z, y))"
   ]
  }
 ],
 "metadata": {
  "date": 1602643870.398518,
  "filename": "38_optimization.rst",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "title": "Foundations of Computational Economics #38",
  "vscode": {
   "interpreter": {
    "hash": "7e955f0f740a88143167821a8b0fbd15a506d043cd936f8554468bc13302d823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
