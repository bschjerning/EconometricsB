{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Binary reponses\n",
    "\n",
    "### Econometrics B (ØkB)\n",
    "\n",
    "Wooldridge (2010, Ch. 15)\n",
    "\n",
    "Bertel Schjerning\n",
    "\n",
    "Department of Economics, University of Copenhagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary Response Models  \n",
    "\n",
    "**Binary outcomes** occur in many real-world decisions where $y \\in \\{0, 1\\}$:\n",
    "\n",
    "- **Labor force participation:** Will an individual work or not?  \n",
    "- **Consumer choice:** Will a consumer buy a car or not?  \n",
    "- **Commuter behavior:** Will a commuter use public transit or not?  \n",
    "- **Residential decisions:** Will a household move or stay?  \n",
    "- **College enrollment:** Will a student attend college or not?  \n",
    "- **Firm behavior:** Will a firm invest, export, or issue a patent?  \n",
    "- **Student decision:** Will a student take the exam?\n",
    "\n",
    "Binary outcomes are foundational in **econometric models**, including:\n",
    "\n",
    "- Discrete choice models (e.g. condtional/multinomial logit)\n",
    "- Corner solution models and censored regression (e.g., Tobit)  \n",
    "- Sample selection (e.g., Heckman model)  \n",
    "- Discrete-continuous choice \n",
    "- Duration models (e.g., time until an event)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Response Probability  \n",
    "\n",
    "The response probability function is defined as:\n",
    "\n",
    "$$\n",
    "p\\left( \\mathbf{x} \\right) \\equiv P\\left( y = 1 \\mid \\mathbf{x} \\right) \n",
    "= P\\left( y = 1 \\mid x_{1}, x_{2}, \\dots, x_{K} \\right)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $y \\in \\{0, 1\\}$ is the binary response (e.g., work vs. not work, buy vs. not buy).  \n",
    "- $\\mathbf{x} \\equiv (x_{1}, x_{2}, \\dots, x_{K})$ is a vector of explanatory variables.\n",
    "\n",
    "For a **continuous** explanatory variable $x_{j}$, the **marginal effect** is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial P\\left( y = 1 \\mid \\mathbf{x} \\right)}{\\partial x_{j}} \n",
    "= \\frac{\\partial p\\left( \\mathbf{x} \\right)}{\\partial x_{j}}\n",
    "$$\n",
    "\n",
    "For a **binary** explanatory variable $x_{K}$, the partial effect measures the change when $x_{K}$ switches from 0 to 1:\n",
    "\n",
    "$$\n",
    "P\\left( y = 1 \\mid x_{1}, x_{2}, \\dots, x_{K-1}, 1 \\right) \n",
    "- P\\left( y = 1 \\mid x_{1}, x_{2}, \\dots, x_{K-1}, 0 \\right)\n",
    "$$\n",
    "\n",
    "**Key Point:**  \n",
    "Partial effects depend on $\\mathbf{x}$ due to the **non-linearity** of $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional Distribution  \n",
    "\n",
    "- $y \\in \\{0, 1\\}$ follows a **Bernoulli distribution** conditional on explanatory variables $\\mathbf{x}$.  \n",
    "\n",
    "- The **response probability** is:  \n",
    "$$\n",
    "p(\\mathbf{x}) \\equiv P(y = 1 | \\mathbf{x}) \\quad \\text{and} \\quad P(y = 0 | \\mathbf{x}) = 1 - p(\\mathbf{x})\n",
    "$$  \n",
    "\n",
    "- The **conditional probability mass function (PMF)** is:  \n",
    "$$\n",
    "f(y | \\mathbf{x}) = p(\\mathbf{x})^{y} \\left[ 1 - p(\\mathbf{x}) \\right]^{1-y}\n",
    "$$  \n",
    "\n",
    "This PMF fully characterizes the distribution of $y$ given $\\mathbf{x}$.\n",
    "\n",
    "- The **first two conditional moments** are:  \n",
    "    - **Mean:**  \n",
    "    $$  \n",
    "    E\\left[ y | \\mathbf{x} \\right] = p(\\mathbf{x})  \n",
    "    $$  \n",
    "\n",
    "    - **Variance:**  \n",
    "    $$  \n",
    "    Var\\left( y | \\mathbf{x} \\right) = p(\\mathbf{x}) \\left[ 1 - p(\\mathbf{x}) \\right]  \n",
    "    $$  \n",
    "    This shows that the variance is maximized when $p(\\mathbf{x}) = 0.5$ and shrinks to zero when $p(\\mathbf{x}) = 0$ or $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Specify and Estimate $p(x)$  \n",
    "\n",
    "- **Parametric Model**:  Suppose $p(x) = G(x, \\beta_0)$, where $G(\\cdot)$ is a known function with parameters $\\beta_0$.  \n",
    "\n",
    "- **Data**:  We observe a random sample $\\{y_i, x_i\\}_{i=1, \\dots, N}$, where each unit $i$ is randomly selected.\n",
    "\n",
    "- **Estimation Methods**: If the model is identified, we can estimate $\\beta_0$ using data on $y$ and $x$.\n",
    "\n",
    "    - **Maximum Likelihood Estimation (MLE)**:  \n",
    "      Use the likelihood function:  \n",
    "      $$  \n",
    "      f(y | x; \\beta_0) = G(x, \\beta_0)^y \\left[1 - G(x, \\beta_0)\\right]^{1-y}\n",
    "      $$  \n",
    "      The Maximum Likelihood estimator is:  \n",
    "      $$\n",
    "      \\hat{\\beta}_{\\text{MLE}} = \\arg \\max_{\\beta} \\frac{1}{N} \\sum_{i=1}^{N} \\log f(y_i | x_i; \\beta)\n",
    "      $$\n",
    "\n",
    "    - **Nonlinear Least Squares (NLS)**:  \n",
    "      Use the conditional expectation:  \n",
    "      $$  \n",
    "      E[y | x] = G(x, \\beta_0)\n",
    "      $$  \n",
    "      The NLS estimator is:  \n",
    "      $$\n",
    "      \\hat{\\beta}_{\\text{NLS}} = \\arg \\min_{\\beta} \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - G(x_i, \\beta)\\right)^2\n",
    "      $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLE or NLS?  \n",
    "\n",
    "- **MLE** is asymptotically efficient:  \n",
    "    - Uses all information about the distribution $f(y | x; \\beta_0)$.  \n",
    "    - **But**: It becomes inconsistent if the distribution is misspecified.  \n",
    "\n",
    "- **NLS** only relies on the first moment:  \n",
    "    - Uses $E[y | x] = G(x, \\beta_0)$.  \n",
    "    - Sometimes, NLS is more robust than MLE.  \n",
    "\n",
    "- **Not in this case**:  \n",
    "    - If the model for the conditional density $f(y | x; \\beta_0)$ is misspecified,  \n",
    "      the regression model $E[y | x] = G(x, \\beta_0)$ is also misspecified.  \n",
    "    - Both methods depend on the correct specification of $G(x, \\beta_0)$.  \n",
    "    - If MLE is inconsistent, NLS is likely inconsistent too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### How to Specify $G(x, \\beta_0)$?  \n",
    "\n",
    "There are several **parametric specifications** for $G(x, \\beta_0)$:\n",
    "\n",
    "1. **Linear Probability Model (LPM)**:  \n",
    "   $$  \n",
    "   P(y = 1 | x) = G(x, \\beta_0) = x \\beta_0  \n",
    "   $$  \n",
    "\n",
    "2. **Linear Index Models**:  \n",
    "   $$  \n",
    "   P(y = 1 | x) = G(x \\beta_0), \\quad \\text{with} \\quad 0 < G(z) < 1  \n",
    "   $$  \n",
    "   Common examples:  \n",
    "   - Logit: $G(z) = \\frac{1}{1 + e^{-z}}$  \n",
    "   - Probit: $G(z) = \\Phi(z)$, where $\\Phi$ is the standard normal CDF.\n",
    "\n",
    "3. **More general parametric models are feasible**:  \n",
    "   $$  \n",
    "   P(y = 1 | x) = G(x, \\beta_0)  \n",
    "   $$  \n",
    "\n",
    "\n",
    "$G$ is often derived from a **latent variable model** or a **random utility model (RUM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Probability Model (LPM)  \n",
    "$$\n",
    "P(y = 1 | x) = E(y | x) = G(x, \\beta_0) = x \\beta_0\n",
    "$$  \n",
    "#### Advantages: \n",
    "* Simple linear regression model.  \n",
    "* Estimable using **OLS**:  \n",
    "  $$  \n",
    "  \\hat{\\beta} = (x'x)^{-1} x'y\n",
    "  $$  \n",
    "* Approximates the **average partial effect (APE)** well.\n",
    "#### Issues:  \n",
    "* Predictions may fall outside the $[0, 1]$ interval.  \n",
    "* **Heteroscedasticity**:  $  \n",
    "  Var(y | x) = x \\beta_0 (1 - x \\beta_0)\n",
    "  $  \n",
    "  - Conditional variance depends on x\n",
    "  - OLS is inefficient and gives invalid standard errors.\n",
    "\n",
    "* Use a **heteroscedasticity-robust** variance matrix (see Equation 12.52 for NLS).\n",
    "\n",
    "* **Negative variance**: Makes **WLS** infeasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment to install if not yet installed\n",
    "# !pip install tabulate\n",
    "# !pip install wooldridge\n",
    "# !pip install statsmodels\n",
    "\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import numpy.random as random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as la\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simulate data from binary response model and estimate LPM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate binary response data from probit model\n",
    "n = 10000  # Number of observations\n",
    "beta = np.array([0, 1])  # Parameters\n",
    "\n",
    "# Generate design matrix (constant + explanatory variable)\n",
    "rng = random.default_rng(seed=43)\n",
    "x1 = rng.normal(0, 1, (n, len(beta) - 1))\n",
    "const = np.ones((n, 1))\n",
    "x_sim = np.concatenate((const, x1), axis=1)\n",
    "\n",
    "# Define binary response simulation function\n",
    "def sim_y(x, beta, rng=random.default_rng()):\n",
    "    beta = beta.reshape(-1, 1) # k x 1\n",
    "    u = rng.normal(0, 1, (x.shape[0], 1))  # Random noise\n",
    "    y = (x @ beta + u > 0).astype(float)   # Binary response: 0 or 1\n",
    "    return y\n",
    "\n",
    "# Generate the response variable\n",
    "y_sim = sim_y(x_sim, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot data, response probability fit, conditional variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_lpmfit(y, x, beta): \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    \n",
    "    # OLS estimate\n",
    "    bhat = la.inv(x.T @ x) @ x.T @ y\n",
    "    x1_grid = np.linspace(-2, 2, 100)\n",
    "\n",
    "    # Plot 1: Data and LPM fit\n",
    "    axs[0].scatter(x[:, 1], y, alpha=0.6, label='Data')\n",
    "    axs[0].plot(x1_grid, bhat[0] + x1_grid * bhat[1], color='red', label='LPM fit')\n",
    "    axs[0].set(xlabel='x1', ylabel='y', title='Model fit vs. Data')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot 2: True CDF vs. LPM fit\n",
    "    axs[1].plot(x1_grid, norm.cdf(beta[0] + x1_grid * beta[1]), label='True CDF')\n",
    "    axs[1].plot(x1_grid, bhat[0] + x1_grid * bhat[1], label='LPM fit', color='red')\n",
    "    axs[1].set(xlabel='x1', ylabel='P(y=1 | x1)', title='LPM vs. True P(y=1 | x)')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Plot 3: Predicted variance\n",
    "    xbhat = bhat[0] + x[:, 1] * bhat[1]\n",
    "    axs[2].scatter(x[:, 1], xbhat * (1 - xbhat), color='green', alpha=0.6)\n",
    "    axs[2].set(xlabel='x1', ylabel='Var(y | x)', title='Predicted Var(y | x)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Graphical Illustration of Issues with LPM  \n",
    "\n",
    "- **Predicted probability** $P(y = 1 | x) = x \\beta$ can exceed $[0, 1]$  \n",
    "- **Conditional variance** $Var(y | x) = x \\beta (1 - x \\beta)$ depends on $x$ (heteroscedasticity)  \n",
    "- **Negative variance** occurs when $x \\beta$ is outside $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "beta = np.array([[0, 1]]).T  # try changing parameters\n",
    "y_sim = sim_y(x_sim, beta)\n",
    "plot_lpmfit(y_sim, x_sim, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Replicate results on Married women's labor force participation\n",
    "(Example 15.1 Wooldridge (p. 563-564))\n",
    "\n",
    "Read in data and summarize key variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import wooldridge\n",
    "df = wooldridge.data('mroz')\n",
    "y = df['inlf']  # Binary response variable: Labor force participation\n",
    "xvars=['nwifeinc','educ', 'exper','expersq','age', 'kidslt6', 'kidsge6']\n",
    "x = df[xvars].copy()  \n",
    "x['const'] = 1.0 # Add constant column of 1s\n",
    "\n",
    "# describe\n",
    "df[['inlf']+xvars].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling married women's Labor force participation using LPM\n",
    "- Estimator for lpm is very simple (OLS)\n",
    "- Need to use heteroscedasticity robust standard errors (nice exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate LPM coefficients using OLS\n",
    "bhat_lpm = la.inv(x.T @ x) @ x.T @ y\n",
    "\n",
    "# Display results in a table\n",
    "print(tabulate({\"Variable\": x.columns, \"bhat (LPM)\": bhat_lpm}, headers=\"keys\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LPM estimated with OLS using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# You can also estimate linear regression models using canned routines from statsmodels\n",
    "mod = sm.OLS(y, x)\n",
    "out_lpm = mod.fit(cov_type='HC0') # use heteroscedasticity robust standard errors (HC0)\n",
    "print(out_lpm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Variable Model (LVM)\n",
    "\n",
    "**Latent outcome variable, $y^*$**\n",
    "\\begin{align*}\n",
    "y^*=x\\beta _{0}+\\varepsilon\n",
    "\\end{align*}\n",
    "\n",
    "**Observed outcome, $y \\in \\{0, 1\\}$:**  \n",
    "\\begin{align*}\n",
    "y = \\mathbb{1}(y^{\\ast }>0)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "**Response probability**\n",
    "\\begin{align*}\n",
    "P(y=1|x) &= P( y^{*}>0|x)  \\\\\n",
    "&=  P(\\varepsilon >-x\\beta_{0}|x) \\\\\n",
    "&=1-P(\\varepsilon <-x\\beta_{0}|x) \\\\\n",
    "&=1-G( -x\\beta _{0}) \\text{... $G$ is CDF of $\\varepsilon$ conditional on $x$} \\\\\n",
    "&=  G( x\\beta _{0})  \\text{... if $G$ is symmetric around zero}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Symmetric distribution\n",
    "\n",
    "![title](img/symmetric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Identification Issues\n",
    "over all scale of **$\\beta$ cannot be identified in the latent variable model**\n",
    "\n",
    "\\begin{align*}\n",
    "    P\\left( y=1|x\\right) &=P\\left( xc\\beta _{0}+c\\varepsilon >0|x\\right) \\\\ \n",
    "    &=P\\left( x\\beta _{0}+\\varepsilon >0|x\\right)  \n",
    "\\end{align*}\n",
    "\n",
    "* Choice probability $P\\left( y=1|x\\right) $ does not change if e.g. $\\tilde{\\beta}=c\\beta _{0}$, $\\tilde{\\varepsilon}=c\\varepsilon $ \n",
    "* Sample objective function can be maximized for a continuum of values of $c$\n",
    "* $\\beta _{0}$ is only identified up to a scale factor. \n",
    "\n",
    "**Solution:** Normalize distribution of $\\varepsilon $ by for example by assuming fixed variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Identification – Probit Example  \n",
    "\n",
    "**Latent variable model with normal errors:**  \n",
    "$$\n",
    "y = \\mathbb{1}(x \\beta_0 + \\varepsilon > 0), \\quad \\varepsilon | x \\sim N(0, \\sigma_0^2)\n",
    "$$\n",
    "\n",
    "**Response probability:**  \n",
    "\\begin{aligned}\n",
    "P(y = 1 | x) &= P(\\varepsilon > -x \\beta_0 | x) \\\\\n",
    "&= 1 - P(\\varepsilon < -x \\beta_0 | x) \\\\\n",
    "&= 1 - P\\left(\\frac{\\varepsilon}{\\sigma_0} < \\frac{-x \\beta_0}{\\sigma_0} \\Bigg| x\\right) \\\\\n",
    "&= 1 - \\Phi\\left(-\\frac{x \\beta_0}{\\sigma_0}\\right) \\quad \\text{(since $\\varepsilon / \\sigma_0 \\sim N(0, 1)$)} \\\\\n",
    "&= \\Phi\\left(\\frac{x \\beta_0}{\\sigma_0}\\right) \\quad \\text{(symmetry of normal distribution)}\n",
    "\\end{aligned}\n",
    "**Key Point:**  \n",
    "- The response probability depends only on the **relative size** of $ \\beta_0 / \\sigma_0 $.  \n",
    "- This implies that **$\\beta_0$** and **$ \\sigma_0 $** cannot be separately identified – only the ratio $ \\beta_0 / \\sigma_0$ is identified in the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Identification – Probit Example  \n",
    "\n",
    "* We usually apply the scale normalization $ \\text{Var}(\\varepsilon | x) = \\sigma_0 = 1 $  \n",
    "\n",
    "$$\n",
    "P(y = 1 | x) = \\Phi(x \\beta), \\quad \\text{where} \\quad \\beta = \\frac{\\beta_0}{\\sigma_0}\n",
    "$$\n",
    "\n",
    "- After the normalization, $ \\beta $ is identified.\n",
    "- Important to keep in mind when interpreting $ \\beta $ in binary choice models.\n",
    "- The effect of $ x $ *appears* smaller when $ \\text{Var}(\\varepsilon | x) $ is large.\n",
    "- If $ \\beta = \\frac{\\beta_0}{\\sigma_0} $ is identifiable, so is the relative size of two coefficients for explanatory variables $ x_l $ and $ x_k $:\n",
    "\n",
    "$$\n",
    "\\frac{\\beta^l_0 / \\sigma_0}{\\beta^k_0 / \\sigma_0} = \\frac{\\beta^l_0}{\\beta^k_0}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probit and Logit are two very popular specifications of $G$  \n",
    "\n",
    "**Probit:** $G$ is the CDF of the standard *normal* distribution  \n",
    "\n",
    "$$\n",
    "G(z) = \\Phi(z)\n",
    "$$\n",
    "\n",
    "The derivative of $G$ is the standard normal density:\n",
    "\\begin{aligned}\n",
    "g(z) &= \\frac{d\\Phi(z)}{dz} = \\phi(z)\n",
    "= (2\\pi)^{-1/2} \\exp(-z^2 / 2) \n",
    "\\end{aligned}\n",
    "\n",
    "**Logit:** $G$ is the CDF of the standard *logistic* distribution  \n",
    "\n",
    "$$\n",
    "G(z) = \\Lambda(z) = \\frac{\\exp(z)}{1 + \\exp(z)} = \\frac{1}{1 + \\exp(-z)}\n",
    "$$\n",
    "\n",
    "The derivative of $G$ is the standard logistic density:\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{d\\Lambda(z)}{dz} = \\frac{\\exp(z)}{[1 + \\exp(z)]^2} = \\frac{\\exp(-z)}{[1 + \\exp(-z)]^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def G(z, model='probit'):\n",
    "    # Index function for probit and logit models\n",
    "    if model=='probit':\n",
    "        return norm.cdf(z)\n",
    "    elif model=='logit':\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "def g(z, model='probit'):\n",
    "    # Derivative: g(z)=dG(z)/dz\n",
    "    if model=='probit':\n",
    "        return norm.pdf(z)\n",
    "    elif model=='logit':\n",
    "        z=-np.abs(z) # use transformation to avoid overflow (ok since g is symmetric)\n",
    "        return np.exp(z)/(1+np.exp(z))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some graphs \n",
    "(output: see next slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_G(scale=1): \n",
    "    # If scale=1 plot_G plots the cdf and pdf for standard normal and standard logistic\n",
    "    # If scale is specified, variance of logistic distribution is scaled by 1/scale. '\n",
    "    # If scale=1/(np.pi/np.sqrt(3)) the variance of logistic is 1 (like standard normal)\n",
    "    # If scale=0.25/0.4, logistic is calibrated to have identical density at z=0. i.e. g(0)=0.4 (like standard normal, 0.25 for standard logistic)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16,8))\n",
    "    z=np.linspace(-4,4,100);  # grid over z (for plotting)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    \n",
    "    # plot 1: CDF Logit and Probit\n",
    "    axs[0].plot(z, G(z, 'probit')) \n",
    "    axs[0].plot(z, G(z/scale, 'logit')) \n",
    "    axs[0].set(xlabel='z', ylabel='G(z)', title='CDF'); \n",
    "    axs[0].legend(('Probit', 'Logit'), loc='lower right')\n",
    "\n",
    "    # plot 2: PDF Logit and Probit\n",
    "    axs[1].plot(z, g(z, 'probit')) \n",
    "    axs[1].plot(z, 1/scale*g(z/scale, 'logit')) \n",
    "    axs[1].set(xlabel='z', ylabel='g(z)', title='PDF'); \n",
    "    axs[1].legend(('Probit', 'Logit'), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unscaled distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_G(scale=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Rescaled distributions\n",
    "Calibrate to have identical density at 0 \n",
    "\n",
    "When unscaled density is 0.4 for standard normal and 0.25 for standard logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_G(scale=0.25/0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probit or Logit\n",
    "Both these choices implies $0<G(z)<1$ so that \n",
    "1. Predictions of $P(y=1|x)$ are never outside the unit interval \n",
    "1. Condtional variance $Var(y|x)$ is always positve\n",
    "\n",
    "Logistic and normal distributions are very similar\n",
    "- Both symmetric\n",
    "- Logistic distribution have longer tails\n",
    "- The derivatibes $g(z)\\equiv dG(z)/dz>0$  are both positive over infinite domain\n",
    "\n",
    "Scale is different\n",
    "- **Probit:** std. dev is $\\sigma=1$\n",
    "- **Logit :** std. dev is $\\sigma \\simeq \\pi/\\sqrt(3)=1.8137993642342178$\n",
    "- but scale differences are irrelevant (and unidentified)\n",
    "- .... and average partial effects and the ratio $\\hat{\\beta}_l/\\hat{\\beta}_k$ are similar across specifications\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimation: Likelihood Contributions and Derivatives  \n",
    "\n",
    "Given a random sample $\\{ y_{i}, x_{i} \\}_{i=1, \\dots, N}$, we can compute the density of $y_i$ conditional on $x_i$:\n",
    "\n",
    "$$\n",
    "f(y_i | x_i; \\beta) = G(x_i \\beta)^{y_i} [1 - G(x_i \\beta)]^{1 - y_i}\n",
    "$$\n",
    "\n",
    "The **log-likelihood** for observation $i$ is:\n",
    "\n",
    "$$\n",
    "\\ell_i(\\beta) = y_i \\log[G(x_i \\beta)] + (1 - y_i) \\log[1 - G(x_i \\beta)]\n",
    "$$\n",
    "\n",
    "The **sample log-likelihood function** is:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\beta) = N^{-1} \\sum_{i=1}^{N} \\ell_i(\\beta)\n",
    "$$\n",
    "\n",
    "The **$1 \\times K$ vector of derivatives** for the conditional likelihood contribution is:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\beta} \\ell_i(\\beta) &= \\frac{g(x_i \\beta) x_i y_i}{G(x_i \\beta)} - \\frac{g(x_i \\beta) x_i (1 - y_i)}{1 - G(x_i \\beta)} \\\\\n",
    "&= \\frac{g(x_i \\beta) x_i [y_i - G(x_i \\beta)]}{G(x_i \\beta)(1 - G(x_i \\beta))}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Implementation of log likelihood contributions and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logl_i(y, x, beta, model='probit', out='logl'):\n",
    "    beta=np.array(beta).reshape(-1,1)\n",
    "    xb=x@beta        # Linear index xb=x1*b1+x2*b2+...+xK*bK\n",
    "    Gx=G(xb, model)  # Response probability at x\n",
    "    Gx=np.minimum(np.maximum(Gx,1e-15),1-1e-15)\n",
    "\n",
    "    logl = np.log(Gx)*y + np.log(1-Gx)*(1-y)     # Nx1 vector of log-likelihood contributions\n",
    "    if out=='logl': \n",
    "        return logl\n",
    "    \n",
    "    # Compute derivarives of logl wrt beta\n",
    "    gx=g(xb, model)                              # Density at xb\n",
    "    dlogl=gx*x*(y-Gx)/( Gx* (1-Gx))              # NxK matrix of derivatives of logl\n",
    " \n",
    "    if out=='dlogl': \n",
    "        return dlogl\n",
    "\n",
    "    if out=='all':         \n",
    "        return xb, Gx, gx, logl, dlogl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood (an M-Estimator)  \n",
    "\n",
    "We can easily reformulate the maximization problem in MLE as a minimization problem:  \n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}_{MLE} &= \\arg \\max_{\\beta} N^{-1} \\sum_{i=1}^{N} \\log f(y_i | x_i; \\beta) \\\\\n",
    "&= \\arg \\min_{\\theta} N^{-1} \\sum_{i=1}^{N} q(w_i, \\theta)\n",
    "\\end{aligned}\n",
    "\n",
    "Where:\n",
    "- $w_i = (x_i, y_i)$ is the observed data for unit $i$\n",
    "- $\\theta = \\beta$ represents the parameters to estimate\n",
    "- $q(w_i, \\theta) = -\\log f(y_i | x_i; \\beta) = -\\ell_i(\\beta)$ is the objective function whose expectation we aim to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Implementation of $\\hat{\\beta}_{MLE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def estim_mle(y, x, model, use_grad=1):\n",
    "    x = np.array(x)\n",
    "    N,k=x.shape    \n",
    "    y = np.array(y).reshape(N, 1)\n",
    "\n",
    "    # objective function is negative of mean log-likelihood \n",
    "    q=lambda beta:    - np.mean(logl_i(y, x,beta, model));\n",
    "\n",
    "    beta_start = np.zeros(k)  # use vector of zeros starting values   \n",
    "    \n",
    "    if use_grad==0: \n",
    "        out=minimize(q, x0=beta_start) \n",
    "    \n",
    "    if use_grad==1: \n",
    "        grad=lambda beta: - np.mean(logl_i(y, x,beta, model, out='dlogl'), axis=0);\n",
    "        out=minimize(q, x0=beta_start, jac=grad)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output from minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model='logit'\n",
    "out=estim_mle(y, x, model, use_grad=0) # try use_grad=0 - what changes? \n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output from minimize converted into estimation output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model='logit'\n",
    "out=estim_mle(y, x, model, use_grad=1)\n",
    "print(out.keys())\n",
    "print('messsage:', out.message)\n",
    "print('minimize used %d iterations, %d function evaluations and %d evaluations of the jacobian' %  \n",
    "      (out.nit, out.nfev, out.njev))\n",
    "N=len(y)\n",
    "se=np.sqrt(np.diag(out.hess_inv/N));  # standard errors (square rooot of diaginal of Avar=Hinv/N)\n",
    "table={\"\":x.columns, \"bhat \": out.x, \"se \":  se , \"t-value \": out.x/se}\n",
    "print('\\nParameter estimates from', model, '\\n',tabulate(table, headers=\"keys\",floatfmt=\"10.4f\"))\n",
    "print('mean logl %g' % -out.fun)\n",
    "print('\\ng*Hinv*g.T: Chi-Square test for gradients beein zero\\n', out.jac@ out.hess_inv @out.jac.T)\n",
    "print('\\njac: derivative of (negative of) likehood wrt beta\\n', out.jac)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Inference Setup for M-Estimators and MLE\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta \\in \\Theta} \\frac{1}{N} \\sum_{i=1}^{N} q(w_i, \\theta) \\quad \\text{(M-estimator)}\n",
    "$$  \n",
    "\n",
    "Under regular conditions, $\\hat{\\theta}$ is $\\sqrt{N}$-asymptotically normal:\n",
    "\n",
    "$$\n",
    "\\sqrt{N}(\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} N \\left( 0, A_0^{-1} B_0 A_0^{-1} \\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "A_0 = E \\left[ H_q(w_i, \\theta_0) \\right], \\quad \n",
    "B_0 = E \\left[ s(w_i, \\theta_0) s(w_i, \\theta_0)' \\right]\n",
    "$$\n",
    "\n",
    "and \n",
    "$$ s(w, \\theta) = \\frac{\\partial}{\\partial \\theta} q(w, \\theta), \\quad (P \\times 1)\n",
    "$$\n",
    "$$\n",
    "H_q(w, \\theta) = \\frac{\\partial^2}{\\partial \\theta \\partial \\theta'} q(w, \\theta), \\quad (P \\times P)\n",
    "$$\n",
    "\n",
    "- How to estimate $\\text{Avar}(\\hat{\\theta}) = A_0^{-1} B_0 A_0^{-1} / N$?\n",
    "- Keep in mind that $q(w_i, \\theta) = -\\log f(y_i | x_i; \\beta) = -\\ell_i(\\beta)$, so $A_0=E[H_q(x_i, \\beta_0)]=-E[H_{logl}(x_i, \\beta_0)]$\n",
    "- For MLE, the information identity ensures that $B_0=A_0$ if the model is well specified so that $\\text{Avar}(\\hat{\\theta}) = A_0^{-1}/ N$\n",
    "- How to consistently estimate $A_0$ and/or $B_0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "For inference, we use the expectation of the Hessian (valid due to information identidity):  \n",
    "$$\n",
    "\\text{Avar}(\\hat{\\beta}) = A_0/N = -E[H(x_i, \\beta_0)]/N\n",
    "$$\n",
    "\n",
    "To obtain the $K \\times K$ **Hessian matrix** of the likelihood, we differentiate the score function once more:\n",
    "\n",
    "$$\n",
    "H(x_i, \\beta) = -\\frac{g(x_i, \\beta)^2 x'_i x_i}{G(x_i, \\beta)(1 - G(x_i, \\beta))} + [y_i - G(x_i, \\beta)]L(x_i, \\beta)\n",
    "$$\n",
    "\n",
    "Here, $L(x_i, \\beta)$ is a complicated function of $x_i$ but $[y_i - G(x_i, \\beta)]L(x_i, \\beta)$ has expectation 0 when the likelihood is maximized. Thus:\n",
    "\n",
    "$$\n",
    "-E[H(x_i, \\beta_0) | x_i] = A(x_i, \\beta_0) = \\frac{g(x_i, \\beta_0)^2 x'_i x_i}{G(x_i, \\beta_0)(1 - G(x_i, \\beta_0))}\n",
    "$$\n",
    "\n",
    "We can then estimate $\\text{Avar}(\\hat{\\beta})$ as:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{Avar}(\\hat{\\beta})} = \\left\\{ N^{-1} \\sum_{i=1}^N A(x_i, \\beta) \\right\\}^{-1}/N = \n",
    "\\left\\{ \\sum_{i=1}^N \\frac{g(x_i, \\hat{\\beta})^2 x'_i x_i}{G(x_i, \\hat{\\beta})(1 - G(x_i, \\hat{\\beta}))} \\right\\}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python Implementation $\\widehat{\\text{Avar}(\\hat{\\beta})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives of likelihood contribution\n",
    "def avar(y, x, beta, model, cov_type='Ainv'):\n",
    "    x = np.array(x)\n",
    "    N,k=x.shape    \n",
    "    y = np.array(y).reshape(N, 1)\n",
    "    beta = np.array(beta).reshape(k, 1)\n",
    "\n",
    "    xb, Gx, gx, logl, dlogl  = logl_i(y, x, beta, model, out='all')\n",
    "    A=(gx*x).T @(gx*x/(Gx* (1-Gx)))\n",
    "    if cov_type=='Ainv':\n",
    "        return la.inv(A)    \n",
    "\n",
    "    B=(dlogl).T @ dlogl # equals outer product of scores [sum_i(s_i*s_i')]\n",
    "    if cov_type=='Binv':\n",
    "        return la.inv(B)\n",
    "    if cov_type=='sandwich':\n",
    "        return la.inv(A) @ B @ la.inv(A)\n",
    "\n",
    "bhat=out.x\n",
    "print('Standard errors')\n",
    "for cov_type in ['Ainv', 'Binv', 'sandwich']:\n",
    "    V=avar(y, x, bhat, model, cov_type)\n",
    "    se= np.sqrt(np.diag(V))\n",
    "    print(cov_type, se.round(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compare results with and without analytical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Output from minimize with and without derivatives\n",
    "res_p_nograd=estim_mle(y,x,model, use_grad=0)\n",
    "res_p=estim_mle(y,x,model, use_grad=1)\n",
    "print('number of iterations - with gradients    : ', res_p.nit)\n",
    "print('number of iterations - without gradients : ', res_p_nograd.nit)\n",
    "print('number of function evaluations - with gradients   : ', res_p.nfev)\n",
    "print('number of function evaluations - without gradients: ', res_p_nograd.nfev)\n",
    "print('number of function evaluations - without gradients/(1+# of parameters): ', res_p_nograd.nfev/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Replicate results in Example 15.2 (Wooldridge, p. 580)\n",
    "Table 15.1: LPM, Logit and Probit Estimates of of Labor Force Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# LPM estimated by OLS: \n",
    "beta_lpm=la.inv(x.T@x)@x.T@y  \n",
    "\n",
    "# Estimate probit and logit using MLE \n",
    "for model in [\"probit\", \"logit\"]:\n",
    "    out[model]=estim_mle(y,x,model)\n",
    "    out[model].beta=out[model].x\n",
    "    out[model].avar=avar(y, x, out[model].beta, model, cov_type='Ainv')\n",
    "    out[model].se=np.sqrt(np.diag(out[model].avar))\n",
    "    \n",
    "# Table with parameter estimates\n",
    "table={\"\":x.columns,\"bhat (lpm)\": beta_lpm, \"se(lpm)\": out_lpm.bse, \n",
    "       \"bhat (probit)\": out['probit'].beta, \"se(probit)\": out['probit'].se,  \n",
    "       \"bhat (logit)\": out['logit'].beta, \"se(logit)\": out['logit'].se}  \n",
    "print('\\nParameter Estimates\\n', tabulate(table, headers=\"keys\",floatfmt=\"10.3f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "# Log-likelihood and Pseudo-R-squared\n",
    "const=np.array(x['const']).reshape(-1,1)\n",
    "N=len(y)\n",
    "for model in [\"probit\", \"logit\"]:\n",
    "    onlyconst=estim_mle(y,const,model, use_grad=1)\n",
    "    out[model].logl0=-onlyconst.fun*N        # log-likelihood at restricted model (only constant)\n",
    "\n",
    "    out[model].logl=-out[model].fun*N  # log-likelihood at unrestricted model (estimated above)\n",
    "    out[model].R2=1.0-out[model].logl/out[model].logl0 # Pseudo R-squred\n",
    "\n",
    "    \n",
    "r=np.array(y)-np.array(x)@bhat_lpm\n",
    "R2_lpm=1-sum(r**2)/sum((y-np.mean(y))**2)\n",
    "df=out[model].beta.size-1 # degrees of freedom for LR test\n",
    "crit=chi2.ppf(0.95,df) # cirtical value (5 pct significance level)\n",
    "print('crit', crit)\n",
    "\n",
    "table={\"\":[\"Log-likelihood\", \"Log-likelihood (only constant)\",  \"LR test\", \"Chi Square Critical value (df=%d)\" % df, \"Pseudo R^2\"], \n",
    "       \"lpm \" :   [\"\", \"\", \"\", \"\", R2_lpm.round(5)],\n",
    "       \"logit \" : [out['logit'].logl, out['logit'].logl0, 2*(out['logit'].logl-out['logit'].logl0), crit, out['logit'].R2 ],\n",
    "       \"probit \" :[out['probit'].logl, out['probit'].logl0, 2*(out['probit'].logl-out['probit'].logl0), crit, out['probit'].R2]}\n",
    "\n",
    "print(tabulate(table, headers=\"keys\",floatfmt=\"10.3f\"))\n",
    "print(\"\\nNumber of observartions, N=%i\\n\" % N)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial Effects - computation\n",
    "\n",
    "**Discrete change**\n",
    "\n",
    "\\begin{equation*}\n",
    "PE_{\\Delta x_{k}}\\left( x^{0}\\right) =P\\left(\n",
    "y=1|x_{1}^{0},x_{2}^{0},..,x_{k}^{0}+\\Delta x_{k}^{0},..,x_{K}^{0}\\right)\n",
    "-P\\left( y=1|x^{0}\\right) \n",
    "\\end{equation*}\n",
    "\n",
    "If $x_{k}$ is a dummy variable: $x_{k}=0$, $\\Delta x_{k}=1$%\n",
    "\n",
    "Remark: If $x_{k}$ is continuous, then we often take effect relative to $%\n",
    "\\Delta x_{k}$.%\n",
    "\n",
    "**Marginal Change**\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left. \\frac{\\partial P\\left( y=1|x\\right) }{\\partial x_{k}}\\right \\vert\n",
    "_{x=x^{0}}=\\underset{\\text{density, }>0}{\\underbrace{g\\left( x^{0}\\beta \\right) }}\\beta _{k}\\text{, where }g\\left( z\\right) =\\frac{\\partial G}{%\n",
    "\\partial z}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial Effects - identification\n",
    "**Scale of $\\beta $ or $G$ is irrelevant for the partial effects.**\n",
    "\n",
    "Suppose $P\\left( y=1|x\\right) =G\\left( x\\beta \\right) =\\tilde{G}\\left(\n",
    "x\\beta /\\sigma \\right) $\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left. \\frac{\\partial P\\left( y=1|x\\right) }{\\partial x_{k}}\\right \\vert\n",
    "_{x=x^{0}}=\\underset{\\text{density, }>0}{\\underbrace{\\tilde{g}\\left(\n",
    "x^{0 }\\beta /\\sigma \\right) }}\\beta _{k}/\\sigma \\text{, where }\\tilde{g} \\left( z\\right) =\\frac{\\partial \\tilde{G}}{\\partial z}\n",
    "\\end{equation*}\n",
    "\n",
    "- Partial effect only depends on $\\beta /\\sigma $- which is what we can\n",
    "identify\n",
    "- Hence, the normalization $\\sigma =1$ is without loss of generality - if interest is in the partial effect\n",
    "- $\\beta _{j}$ determines the sign of partial effect (in binary choice\n",
    "models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial Effects\n",
    "\n",
    "In LPM - partial effects are always equal to $\\beta _{j}$\n",
    "\n",
    "In LVM - $\\beta _{j}$ determines the sign of partial effects - but magnitude depends on $x^{0}$\n",
    "\n",
    "Which $x^{0}$ to choose?\n",
    "\n",
    "1. $x^{0}$ sample mean of $x$\n",
    "2. sample enumeration (evaluate PE at the data and take the sample average)\n",
    "\n",
    "APE and PEA may be very different since $p(x) $ is nonlinear (example on next slide)\n",
    "- 1) estimates *partial effect at the average* (PEA)\n",
    "- 2) estimates *average partial effect* (APE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial effect as function of $x_1$\n",
    "Suppose $\\beta _{1}<0$\n",
    "\n",
    "![title](img/partial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LPM is often a good approximation of APE\n",
    "* $\\hat{\\beta}_{LPM}=(x'x)^{-1}x'y$ can be a good approximation of average partial effects (APE)\n",
    "* if both $x$ and $\\varepsilon$ are normally LPM is asymptotically  equivalent to APE for probit\n",
    "* LPM inadequate if interest lies in the distribution of partial effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate LPM using OLS and compare to APE (using our simulated data from above)\n",
    "beta = np.array([[0, 3]]).T  # try changing parameters\n",
    "y_sim = sim_y(x_sim, beta)\n",
    "bhat_simdata_lpm=la.inv(x_sim.T@x_sim)@x_sim.T@y_sim\n",
    "APE_simdata=np.mean(norm.pdf(x_sim@beta))*beta\n",
    "table={\"beta, true\": beta, \"parameter estimate, bhat_lpm\": bhat_simdata_lpm.round(5), \"Avereage partial effects, APE\": APE_simdata.round(5)}\n",
    "print(tabulate(table, headers=\"keys\"))\n",
    "plot_lpmfit(y_sim, x_sim, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial Effects in our empirical example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate LPM using OLS and compare to APE (using our data on labor force participation)\n",
    "bhat_lpm=la.inv(x.T@x)@x.T@y \n",
    "\n",
    "for model in [\"probit\", \"logit\"]:\n",
    "    out[model].APE = np.mean(g(x@out[model].beta, model))*out[model].beta\n",
    "    out[model].PEA = g(np.mean(x,axis=0)@ out[model].beta, model)*out[model].beta\n",
    "\n",
    "table={\"\":x.columns[1:],\n",
    "        \"lpm\": bhat_lpm[1:], \n",
    "       \"logit\": out['logit'].beta[1:], \n",
    "       \"probit\": out['probit'].beta[1:],\n",
    "       \"APE (logit)\":  out['logit'].APE[1:], \n",
    "       \"APE (probit)\": out['probit'].APE[1:],\n",
    "       \"PEA (logit)\":  out['logit'].PEA[1:], \n",
    "       \"PEA (probit)\": out['probit'].PEA[1:]}\n",
    "\n",
    "print(tabulate(table, headers=\"keys\",floatfmt=\"10.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling of parameters\n",
    "**Logit**\n",
    "\n",
    "$PE_{x_{k}}^{L}\\left( x^{0}\\right) =\\beta _{k}^{L}\\left. \\frac{\\partial \\Lambda }{\\partial z}\\right \\vert _{z=x^{0\\prime }\\beta }=\\beta\n",
    "_{k}^{L}\\frac{\\exp \\left( x^{0}\\beta ^{L}\\right) }{\\left[ 1+\\exp\n",
    "\\left( x^{0 }\\beta ^{L}\\right) \\right] ^{2}}$\n",
    "\n",
    "**Probit:**\n",
    "\n",
    "$PE_{x_{k}}^{P}\\left( x^{0}\\right) =\\beta _{k}^{P}\\phi \\left(\n",
    "x^{0}\\beta ^{P}\\right) = \\beta _{k}^{P} (2\\pi)^{-1/2} \\exp(-(x^{0}\\beta ^{P})^2 / 2)  $\n",
    "\n",
    "\n",
    "We can \"Calibrate\" at coefficients to have same partial effect at $x^0\\beta =0$\n",
    "\\begin{align*}\n",
    "PE_{x_{k}}^{L}\\left( 0\\right) &=&\\beta _{k}^{L}\\frac{1}{4} \\\\\n",
    "PE_{x_{k}}^{L}\\left( 0\\right) &=&\\beta _{k}^{P}\\frac{1}{\\sqrt{2\\pi }}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta _{k}^{L}\\frac{1}{4}\\simeq \\beta _{k}^{P}\\frac{1}{\\sqrt{2\\pi }}\n",
    "\\Rightarrow \\beta _{k}^{L}\\simeq \\beta _{k}^{P}\\frac{4}{\\sqrt{2\\pi }}\n",
    "=1.60\\cdot \\beta _{k}^{P}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=4/np.sqrt(2*np.pi)\n",
    "table={\"\":x.columns,\n",
    "       \"bhat (lpm)\":    bhat_lpm.round(5), \n",
    "       \"bhat (logit)\":  out['logit' ].x ,  \n",
    "       \"bhat (probit)\": out['probit'].x ,\n",
    "       \"bhat (logit) / scale\": out['logit'].x/ scale }\n",
    "print('\\nUnscaled and scaled coefficients\\n', tabulate(table, headers=\"keys\",floatfmt=\"10.3f\"))\n",
    "print('\\nScale', scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The relative size of coefficients on two different variables does not depend on scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table={\"\":x.columns,\n",
    "       \"bhat (lpm)\":    bhat_lpm/bhat_lpm[2], \n",
    "       \"bhat (logit)\":  out['logit' ].x/out['logit' ].x[2] , \n",
    "       \"bhat (probit)\": out['probit'].x/out['probit'].x[2]}\n",
    "print('\\nCoefficients relative to coefficent on educ\\n', tabulate(table, headers=\"keys\",floatfmt=\"10.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some takeaways on LPM, probit and Logit\n",
    "\n",
    "- Probit and logit is very similar\n",
    "    - logistic and normal distributions are very similar (both symmetric, although logistic distribution have longer tails)\n",
    "    - partial effects and ratio between parameter estimates very similar     \n",
    "    - latent variable model requires scale normalization to obtain identification\n",
    "    - usually estimated using MLE\n",
    "\n",
    "- LPM \n",
    "    - easy to estimate (OLS)\n",
    "    - easy to allow for endgeonous variables, unobserved effects, dynamics, etc\n",
    "    - prediction of $P(y=1|x)$ can fall outside [0,1] and Var$(\\varepsilon|x)$ can be negative\n",
    "    - assumes constant partial effects which is very unrealistic\n",
    "    - ....but LPM is often a good approximation of APE\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1602643870.398518,
  "filename": "binary_choice.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "title": "Econometrics B #1"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
