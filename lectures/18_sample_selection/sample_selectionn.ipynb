{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sample selection in regression models\n",
    "\n",
    "\n",
    "### Econometrics B (Ã˜kB)\n",
    "\n",
    "(Wooldridge Ch. 19)\n",
    "\n",
    "Bertel Schjerning\n",
    "\n",
    "Department of Economics, University of Copenhagen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "**Lectures on sample selection**\n",
    "1. Sample selection in regression models\n",
    "1. Exclusion restrictions\n",
    "1. Likelihood models\n",
    "1. Nonparametric bounds: Set vs point identification in the sample selection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection due to sample design\n",
    "- Sample selection based on an explanatory variable\n",
    "\t- Example: only persons aged 30-50 years are interviewed.\n",
    "- Truncation:\n",
    "\t- Sample selected based on the values of the dependent variable.\n",
    "\t- Ex.: We want to explain wealth and only poor people is sampled.\n",
    "\n",
    "### Sample selection due behavior:\n",
    "- *Non-response on survey questions*\n",
    "- *Attrition*: People drop out of the sample over time\n",
    "- *Incidental truncation*: Dependent variable unobserved because of the outcome of another variable (Classical example:  we only observe wages for who work)\n",
    "\n",
    "#### When is sample selection a problem?\n",
    "Problems arises when the selection a non-random drawn from the population of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection framework for regression models\n",
    "Consider the linear regression model\n",
    "\n",
    "$$\ty_{1}=x_{1}\\beta _{1}+u_{1},\\quad \\quad E( u_{1}|x_{1}) =0 $$\n",
    "\n",
    "**The sample selection problem**\n",
    "- $y_{1}$ or $x_{1}$ or both are unobserved when some selection indicator $s=0$\n",
    "- If we run a regression on the selected sample, we effectively condition on $s=1$\n",
    "\n",
    "**Implication**\n",
    "- We will have to work with the regression function $E(y_{1}|x_{1},s=1) $\n",
    "- We need to condition on $s=1$, but object of interest is $E( y_{1}|x_{1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection framework for regression models\n",
    "\n",
    "\n",
    "<img src=\"img/sampleselection.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample selection: 5 cases\n",
    "**Equation of interest:**\n",
    "$$\n",
    "\ty_{1}=x_{1}\\beta _{1}+u_{1},\\quad \\quad E( u_{1}|x_{1}) =0\n",
    "$$\n",
    "\n",
    "We consider **5 types of sample selection:**\n",
    "1. $s$ is a function of $x_{1}$ only\n",
    "1. $s$ is independent of $x_{1}$, and $u_{1}$\n",
    "1. $s=1( a_{1}<y_{1}<a_{2}) $ (truncation)\n",
    "1. $s=1( x\\delta_{2}+v_{2}>0) $ (discrete response selection with dependence between $u_{1}$ and $v_{2}$)\n",
    "\n",
    "1. $y_{2}=\\max (0,x\\delta_{2}+v_{2}) $ and $s=1(y_{2}>0)$ (Tobit selection with dependence between $u_{1}$ and $v_{2}$ - implies more structure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 1: Selection on a regressor\n",
    "**Assume selection is a deterministic function of $x_{1}$ only**\n",
    "\n",
    "$$ s=h\\left( x_{1}\\right) $$\n",
    "\n",
    "Regression model on the selected sample\n",
    "\n",
    "$$ E(y_{1}|x_{1},s=1) =x_{1}\\beta _{1}+E(u_{1}|x_{1},s=1) $$\n",
    "\n",
    "- Since $s$ is a deterministic function of $x_{1}$, $s$ does not give us more information than $x_{1}$\n",
    "\n",
    "$$\tE(u_{1}|x_{1},s=1) =E(u_{1}|x_{1}) $$\n",
    "\n",
    "- Assuming exogenous explanatory variables $E\\left( u_{1}|x_{1}\\right)=0$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\tE(y_{1}|x_{1},s=1)  &=&x_{1}\\beta _{1}+E(u_{1}|x_{1}) \\\\\n",
    "\t&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 1: We can ignore selection when selection is a deterministic function of $x_1$\n",
    "- Selection rule: deterministic function of $x_1$\n",
    "\n",
    "$$ s=h(x_{1}) $$\n",
    "\n",
    "- We can consistently estimate $\\beta $ (as well as $E\\left(y_{1}|x_{1}\\right) $) using OLS on the selected sample, since \n",
    "\n",
    "$$ E\\left( y_{1}|x_{1},s=1\\right) =x_{1}\\beta _{1}$$\n",
    "\n",
    "- **Selection on a regressor is not a problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 2: Selection independent of x and u\n",
    "\n",
    "Assume selection is independent of $x_{1}$ and $u_{1}$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E( y_{1}|x_{1},s=1)  &=&x_{1}\\beta _{1}+E(u_{1}|x_{1},s=1) \\\\\n",
    "\t&=&x_{1}\\beta _{1}+E( u_{1}|x_{1}) \\\\\n",
    "\t&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$\\to$ we can consistently estimate $\\beta $ (as well as $E(y_{1}|x_{1}) $) using OLS on the selected sample, since \n",
    "\n",
    "$$\tE(y_{1}|x_{1},s=1) =x_{1}\\beta _{1} $$\n",
    "\n",
    "**$\\to$ Random sample selection is not a problem**\t\n",
    "-  Example: Sometimes, we (are forced to) make a *random* subsample of our dataset, and the result shows that estimating on our subsample will give us consistent estimates.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Truncated regression: Selection on the response variable (selection on $y_{1}$)\n",
    "\n",
    "Suppose that the selection rule is\n",
    "$$ s=\\mathbb{1}[a_{1} < y_{1} < a_{2}] $$\n",
    "\n",
    "- $\\left( y_{1},x_{1}\\right) $ only observed if $s=1$.\n",
    "- $a_{1}$ and $a_{2}$ are  {known} constants and obviously $a_{2}>a_{1}$.\n",
    "\n",
    "**We do not need to have truncation from both sides**\n",
    "- Truncation only from below corresponds to $a_{2}=\\infty $\n",
    "- Truncation only from above corresponds to $a_{1}=-\\infty $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Selection can't be ignored\n",
    "**Object of interest:** As usual we are interested in estimating \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x_{1}\\right) =x_{1}\\beta _{1} \\\\\n",
    "s=1[ a_{1} < y_{1} < a_{2}] \n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Conditional moment restrictions are not sufficient for identification\n",
    "- Need to specify full conditional distribution of $y_{1}$ given $x_{1}$\n",
    "- Need to estimate using CMLE\n",
    "\n",
    "**Assumption:** Conditional cdf $y_{1}|x_{1}$ is $F(c|x_{1})=P(y_{1}\\leq c|x_{1}) $, where $c$ is the usual dummy argument.\n",
    "\n",
    "- Can't work with distribution of $y_{1}|x_{1}$ directly\n",
    "- Need to derive distribution condition on selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digression: conditional density\n",
    "Recall, the formula for conditional probability\n",
    "$$\n",
    "\tP\\left(A|B\\right) =\\frac{P\\left( A\\cap B\\right) }{P\\left( B\\right) }\n",
    "$$ \n",
    "we can now write\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P\\left( y_{1}\\leq c|x_{1},s=1\\right)  &=&\\frac{P\\left( y_{1}\\leq c,s=1|x_{1}\\right) }{P\\left( s=1|x_{1}\\right) }\\\\\n",
    "&=&\\frac{P\\left( a_{1} < y_{1} < c|x_{1}\\right) }{P\\left(a_{1} < y_{1} < a_{2}|x_{1}\\right) } \\\\\n",
    "&=&\\frac{F\\left( c|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }{F\\left(a_{2}|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }\n",
    "\\end{eqnarray*}\n",
    "\n",
    "To obtain density simply differentiate with respect to $c$\n",
    "$$\n",
    "\tf\\left( c|x_{1},s=1\\right) =\\frac{f\\left( c|x_{1}\\right) }{F\\left(a_{2}|x_{1}\\right) -F\\left( a_{1}|x_{1}\\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood function for truncated regression (selection $y_1$)\n",
    "\n",
    "Plug $y_{1i}$ into this and take the logs, then we arrive at the (average) log likelihood\n",
    "\\begin{eqnarray*}\n",
    "\\ln L(\\beta ,\\sigma)  \t&=&\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln f(y_{1i}|x_{1i},s_{i}=1) \\\\\n",
    "\t\t\t\t&=&\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln \\left( \\frac{f\\left( y_{1i}|x_{1i}\\right) }{F\\left( a_{2}|x_{1i}\\right) -F\\left( a_{1}|x_{1i}\\right) }\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "If $y_{1}|x_{1}\\sim N\\left( x_{1}\\beta _{1},\\sigma ^{2}\\right) $ and $a$ is independent of $x_{1}$ we have the **truncated** Tobit model.\n",
    "$$\n",
    "\\ln L\\left( \\beta ,\\sigma \\right) =\\frac{1}{N}\\sum_{i=1}^{N}s_{i}\\ln \\frac{\\frac{1}{\\sigma }\\phi \\left( x_{1i}\\beta \\right) }{\\Phi \\left( a_{2}\\right) -\\Phi\\left( a_{1}\\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 3: Truncated vs. censored Tobit \n",
    "\n",
    "**Censored Tobit:** we observe covariates $x_{1}$ for all people\n",
    "\n",
    "**Truncated Tobit:** we do not.\n",
    "\n",
    "- If we have data to use the censored Tobit, we would use this, since then we will use all our information\n",
    "\n",
    "- As in censored regression, heteroscedasticity and non-normality has severe consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Incidental truncation (Probit selection)\n",
    "**Structural equation**\n",
    "$$\n",
    "y_{1}=x_{1}\\beta _{1}+u_{1},\\ E\\left( u_{1}|x_{1}\\right) =0\n",
    "$$\n",
    "\n",
    "**Reduced form probit selection equation**\n",
    "\n",
    "$$\n",
    "s=1\\left( x\\delta_{2}+v_{2}>0\\right) \n",
    "\\quad \\quad v_{2}\\sim N\\left( 0,1\\right)\n",
    "\\quad \\quad x=\\left( x_{1},x_{2}\\right)\n",
    "$$\n",
    "- $\\left( u_{1},v_{2}\\right) $ is independent of $x$\n",
    "\n",
    "- We only observe $y_{1}$ when $s=1,$ but $x$ is always observed\n",
    "\n",
    "- Selection equation is a probit equation $P\\left(s=1|x\\right) =\\Phi\\left(x\\delta_{2}\\right)$ \n",
    "\n",
    "\n",
    "\n",
    "- **Assume further that $u_{1},v_{2}$ are mutually mean dependent**,  \n",
    "$E\\left( u_{1}|v_{2}\\right) =\\gamma _{1}v_{2}$\n",
    " \n",
    " $\\to$ This form for dependence rules out many joint distributions\n",
    "\n",
    " $\\to$ but not the bivariate normal distribution.\n",
    "\n",
    " $\\to$ less restrictive than assuming that $u_{1}$ and $v_{2}$ are bivariate normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Case 4: Regression on the selected sample\n",
    "\n",
    "To derive $E\\left( y_{1}|x,s=1\\right) $ we first compute\n",
    "\n",
    "$$\n",
    "E\\left( y_{1}|x,s=1,v_{2}\\right) \n",
    "=x_{1}\\beta _{1}+E\\left( u_{1}|x_{1},v_{2}\\right) \n",
    "$$\n",
    "\n",
    "Because $u_{1}$ and $v_{2}$ are independent of $x$ we can write\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1,v_{2}\\right)  &=&x_{1}\\beta _{1}+E\\left(\n",
    "u_{1}|v_{2}\\right) \n",
    "\\\\\n",
    "&=&x_{1}\\beta _{1}+\\gamma _{1}v_{2}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where we have used the assumption that $E\\left( u_{1}|v_{2}\\right) =\\gamma\n",
    "_{1}v_{2}$\n",
    "\n",
    "**By LIE we obtain the conditional regression function**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta +E\\left( \\gamma\n",
    "_{1}v_{2}|x,s=1\\right) \\\\\n",
    "&=&x_{1}\\beta +\\gamma _{1}E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Regression depends on $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right)$ which takes the form of a mean of a truncated Normal variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: When is selection due to incidental truncation an issue?\n",
    "- If $u_{1}$ and $v_{2}$ are uncorrelated, i.e. if  $\\gamma_{1}=0$\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta _{1}+\\gamma _{1}E\\left(\n",
    "v_{2}|x,v_{2}>-x\\delta_{2}\\right) \\\\\n",
    "&=&x_{1}\\beta _{1}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- There is no selection problem and we can just use OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Incidental truncation (Probit selection)\n",
    "- If $u_{1}$ and $v_{2}$ are correlated, so that $\\gamma_{1}\\neq 0$\n",
    "\n",
    "$$\n",
    "E\\left(y_{1}|x,s=1\\right) \n",
    "=x_{1}\\beta_{1}+\\gamma_{1}E\\left(v_{2}|x,v_{2}>-x\\delta_{2}\\right) \n",
    "$$\n",
    "\n",
    "- OLS on the selected sample is inconsistent due to the selection bias $\n",
    "E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) >0$\n",
    "\n",
    "- $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right)$: omitted variable correlated with $x$\n",
    "\n",
    "- Notice that $v_{2}\\sim N\\left( 0,1\\right)$ \n",
    "\n",
    "- $E\\left( v_{2}|x,v_{2}>-x\\delta_{2}\\right) $ is just the mean of a\n",
    "truncated normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digression: Expected value of a truncated normal\n",
    "\n",
    "First note that when $v_{2}\\sim N\\left( 0,1\\right)$ \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\phi \\left( v_{2}\\right)  &=&\\frac{1}{\\sqrt{2\\pi }}\\exp \\left(-\\frac{v_{2}^{2}}{2}\\right) \\\\\n",
    "\\phi ^{\\prime }\\left( v_{2}\\right) \n",
    "&=&-v_{2}\\frac{1}{\\sqrt{2\\pi }}\\exp \\left( -\\frac{v_{2}^{2}}{2}\\right) \n",
    "=-v_{2}\\phi \\left( v_{2}\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Expected value of a truncated standard normal**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "E\\left( v_{2}|v_{2}>-x\\delta_{2}\\right) \n",
    "&=&\\frac{\\int_{-x\\delta_{2}}^{\\infty }v_{2}\\phi \\left( v_{2}\\right) dv_{2}}{\n",
    "1-\\Phi \\left( -x\\delta_{2}\\right) }\n",
    "=\\frac{-\\int_{-x\\delta_{2}}^{\\infty }\\phi ^{\\prime }\\left( v_{2}\\right)\n",
    "dv_{2}}{1-\\Phi \\left( -x\\delta_{2}\\right) }\\\\\n",
    "&=&\\frac{\\left[ -\\phi \\left( \\infty \\right) --\\phi \\left( -x\\delta\n",
    "_{2}\\right) \\right] }{1-\\Phi \\left( -x\\delta_{2}\\right) }\n",
    "=\\frac{\\phi \\left( x\\delta_{2}\\right) }{\\Phi \\left( x\\delta_{2}\\right) }\n",
    "\\end{eqnarray*}\n",
    "\n",
    " - We use that the normal distribution is symmetric \n",
    "so that $\\phi \\left( x\\delta_{2}\\right) =\\phi \\left( -x\\delta_{2}\\right)$ and $\\Phi \\left( x\\delta_{2}\\right) =1-\\Phi \\left( -x\\delta_{2}\\right)$ \n",
    "\n",
    "- Need to normalize density with probability for truncation so that normalized density integrates to $1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Collecting terms\n",
    "\\begin{eqnarray}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta +\\gamma_{1}\\frac{\\phi \\left(\n",
    "x\\delta_{2}\\right) }{\\Phi \\left( x\\delta_{2}\\right) }  \\notag \\\\\n",
    "&=&x_{1}\\beta +\\gamma_{1}\\lambda \\left( x\\delta_{2}\\right) \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "**Sample selection as an omitted variable problem**\n",
    "\n",
    "- Regressing $y_{1}$ on $x_{1}$ using the selected sample we omit the term $\\lambda \\left( x\\delta_{2}\\right) $ \n",
    "- Heckman suggests: we estimate $\\delta_{2}$, generate $\\lambda \\left( x\\hat{\\delta}_{2}\\right)$ and include it as an regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 4: Heckman's two-step sample selection procedure\n",
    "\n",
    "**Heckit procedure, Heckman (1979):**\n",
    "1. Probit of $s_{i}$ on $x_{i} \\to \\hat{\\delta}_{2}$ and $\\lambda _{i}=\\lambda \\left( x_{i}\\hat{\\delta}_{2}\\right) $\n",
    "2. OLS of $y_{1i}$ on $x_{1i}$ and the generated regressor $\\lambda _{i}=\\lambda \\left( x_{i}\\hat{\\delta}_{2}\\right) \n",
    "\\to \\hat{\\beta}_{1},\\hat{\\gamma}_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test for selectivity bias following Heckman's two-step procedure\n",
    "\n",
    "When $\\gamma_{1}\\neq 0$ *asymptotic inference is complicated* for two reasons\n",
    "1. Heteroscedasticity: $Var\\left( y_{1}|x,s=1\\right) \\neq Var\\left( y_{1}|x\\right)$ is not constant.\n",
    "\n",
    "2. $\\hat{\\lambda}_{i}$ is a generated regressor.\n",
    "\n",
    "- **Non-standard inference when $\\gamma \\neq 0$**\n",
    "Covariance matrix on the form $A_{0}^{-1}B_{0}A_{0}^{-1}$ is robust for heterosedasticity, but this will not solve generated regressors problem\n",
    "- **But we can still test for selectivity bias, i.e. $H_{0}:\\gamma_{1}=0$**\n",
    "(using the usual t-statistics based on OLS standard errors)\n",
    "- **Why?:** Under $H_{0}$ we have $\\gamma_{1}=0$ and $Var\\left(y_{1}|x,s=1\\right) =Var\\left( y_{1}|x\\right) =Var\\left(\n",
    "u_{1}\\right) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to correct standard errors?\n",
    "We can correct for generated regressor problem\n",
    "- Using **asymptotic theory** for **two-step M-estimators** (see 12.5.2) - brain intensive\n",
    "- **Bootstrapping the standard errors** - computer intensive, but easy to do\n",
    "\n",
    "**Alternatively use MLE approach (Partial MLE)**\n",
    "- Stronger assumptions: Need to specify joint distribution of $u_{1}$ and $v_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heteroscedasticity due to selection\n",
    "\n",
    "Showing that $$Var\\left(y_{1}|x,s=1\\right) \\neq Var\\left(y_{1}|x\\right) $$ would take us to far \n",
    "\n",
    "But we can get some intuition by the following example: \n",
    "\n",
    "- Suppose some values of $x$ that imply low wages. \n",
    "- These $x$'s will tend to also imply a lower probability of working \n",
    "- $\\to$ more truncation \n",
    "- $\\to$ a lower variance of the error term in a sample of workers. \n",
    "- $\\to$ variance depends on $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: Tobit Selection: The model\n",
    "**Structural equation**\n",
    "\n",
    "$$\n",
    "y_{1}=x_{1}\\beta _{1}+u_{1},\\  \\ E\\left( u_{1}|x_{1}\\right) =0\n",
    "$$\n",
    "\n",
    "**Selection rule**\n",
    "\\begin{eqnarray*}\n",
    "s &=&1\\left( y_{2}>0\\right) \\\\\n",
    "y_{2} &=&\\max \\left( 0,x\\delta_{2}+v_{2}\\right) \\text{, }\n",
    "\\end{eqnarray*}\n",
    "\n",
    "**Same distributional assumptions as for probit selection**\n",
    "- $x=\\left( x_{1},x_{2}\\right)$\n",
    "- $\\left( u_{1},v_{2}\\right) $ is independent of $x$\n",
    "- $v_{2}\\sim N\\left( 0,\\sigma^2_v\\right) $.\n",
    "- $u_{1},v_{2}$ are mutually dependent with $E\\left( u_{1}|v_{2}\\right)=\\gamma_{1}v_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: Tobit selection- two-step estimation procedure\n",
    "\n",
    "As before\n",
    "$$\n",
    "E\\left( y_{1}|x,s=1\\right) =x_{1}\\beta _{1}+\\gamma_{1}v_{2}\n",
    "$$\n",
    "\n",
    "**This suggest the two-step estimation procedure**\n",
    "1. Estimate $\\delta_{2}$ in the censored Tobit of $y_{2i}$ on $x_{i}$ and compute the residual $\\hat{v}_{2i}$\n",
    "1. Using observations with $y_{2i}>0$ estimate $\\beta _{1}$ and $\\gamma_{1}$ by the OLS regression: \n",
    " \n",
    " $y_{1i}$ on $x_{1i}$ and $\\hat{v}_{2i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 5: A few remarks on Tobit selection\n",
    "\n",
    "**Test for selectivity bias: $H_{0}:\\gamma_{1}=0$**\n",
    "- use t-statistic based on OLS standard errors (valid under the null of no selection).\n",
    "\n",
    "- we can estimate the residuals $\\hat{v}_{2}$ directly\n",
    "\n",
    "  $\\to $we do not have to compute the inverse Mills ratio\n",
    "\n",
    "- **Exclusion restriction not required** allowing $x_{1}=x$ causes no problem , since $v_{2}$ always has separate variation due to the variation in $y_{2}$\n",
    "\n",
    "The vast majority of empirical studies use probit selection rather than Tobit selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exclusion restrictions\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exclusion restrictions (required for probit selection)\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E\\left( y_{1}|x,s=1\\right)  &=&x_{1}\\beta +\\gamma_{1}\\frac{\\phi \\left(\n",
    "x\\delta_{2}\\right) }{\\Phi \\left( x\\delta_{2}\\right) }  \n",
    "\\end{eqnarray}\n",
    "\n",
    "**Exclusion restriction**: \n",
    "- $x_{2}$ is assumed to affect $s$, but not $y_{1}$.\n",
    "- crucial for identification\n",
    "\n",
    "In principle, $\\beta $ is identified in Heckman's sample selection model without an exclusion restriction\n",
    "- BUT only because of the nonlinearity of the inverse Mills ratio.\n",
    "- $\\to$ we do NOT want to heavily rely on a parametric assumption \n",
    "- $\\to$ NEVER estimate Heckman's sample selection model without an exclusion restriction\n",
    "- often the inverse Mills ratio for a large part of its distribution is fairly linear (leading to multicollinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitation: Hard to find exclusion restrictions\n",
    "\n",
    "- I can think of any exclusion restriction, which is not subject to relevant critique\n",
    "- Huge limitation of this approach\n",
    "- Finding exclusion restrictions is an \"art\"\n",
    "\n",
    "**Not possible to validity of exclusion restriction**\n",
    "- Why not simply test whether the instrument, $x_2$, is significant in the structural equation?\n",
    "- This is NOT a validation of the exclusion restriction since we are estimating on the selected sample\n",
    "- even if $x_2$ is insignificant, we cannot be sure that it has no effect in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "- The classical exclusion restriction: Existence of **small children**\n",
    "- **Identifying assumption**: having small children has an effect on women's participation decision, but not on their wage offer\n",
    "\n",
    "\n",
    "- **Critique of exclusion restriction:** \n",
    "\t- Small children can imply that women are restricted to working in jobs with fewer hours and lower wage\n",
    "\t- Having children is an intertemporal decision: future births may depend on current wage offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "Alternative exclusion restriction:\n",
    "- Indicator for guarantied **access to day care** (In Danish \"pasningsgaranti\") or the degree of subsidized day care\n",
    "- Simonsen (2008) considers female labor supply and day care provision. \n",
    "\n",
    "**Critique of exclusion restriction:**\n",
    "- Tiebout sorting model: Most productive women may choose to live where they can be sure to send their children to day care.\n",
    " \n",
    " $\\to$ availability of day care is correlated with both labor supply and wages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Wage equation for women\n",
    "\n",
    "Another classic exclusion restriction: *Husband's income*\n",
    "\n",
    "**Identifying assumption:** Husband reduces wife's labor supply, but has no effect on her wages.\n",
    "\n",
    "\n",
    "\n",
    "**Critique of exclusion restriction:**\n",
    "- Poor exclusion restriction if there is positive assortative matching in the marriage market (high productive men and high productive women match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Exclusion restrictions in dynamic wage equation\n",
    "\n",
    "**Vella and Verbeek (1998):** Wage equation with dynamic labor supply\n",
    "\n",
    "**Exclusion restriction:**\n",
    "- lagged labor market participation\n",
    "\n",
    "**Identifying assumption:** Wage equation is static, but the participation decision is dynamic\n",
    "\n",
    "**Critique of exclusion restriction:**: \n",
    "- no participation previous period due to stress. \n",
    "\n",
    " $\\to$ smaller probability of participating this year due to state dependence\n",
    "- or perhaps since the worker still cannot work due to stress\n",
    "\n",
    " If worker returns: less likely to take-up a stressful job (smaller workload $\\Rightarrow$  lower wage)\n",
    "\n",
    "\n",
    " **Hence, lagged participation may affect current wage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Likelihood Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood models: Need to impose more structure\n",
    "\n",
    "From regression models we can ignore selection if\n",
    "1. $s$ is a deterministic function of $x_{1}$\n",
    "1. selection is independent of $x_{1}$ and $u_{1}$ (or $y_{1}$ given $x_{1}$)\n",
    "\n",
    "For likelihood models \n",
    "- In previous lectures (e.g. on binary response) we worked with the conditional density $f\\left( y_{1}|x_{1};\\theta \\right)$\n",
    "- If neither 1) or 2) are met we need to take explicitly account of the selection in a joint estimation\n",
    "- need to specify $f\\left( y_{1},s|x;\\theta \\right) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood models: Need to impose more structure\n",
    "\n",
    "Structural Model (without selection)\n",
    "\n",
    "$$ f\\left( y_{1}|x_{1};\\theta_{M}\\right) $$\n",
    "\n",
    "Selection model\n",
    "$$\n",
    "f\\left( s|x;\\theta_{S}\\right)\n",
    "$$\n",
    "\n",
    "Selection is ignorable if\n",
    "$$\n",
    "f\\left( y_{1}|x_{1},s\\right) =f\\left( y_{1}|x_{1}\\right)\n",
    "$$\n",
    "\n",
    "This happens if\n",
    "- $s$ is a deterministic function of $x_{1}$\n",
    "- $s$ is independent of $y_{1}$ given $x_{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood models: The joint distribution\n",
    "\n",
    "Joint distribution of $y_{1}$  and $s$\n",
    "\n",
    "$$\n",
    "f\\left( y_{1},s|x_{1};\\theta \\right) =f\\left( y_{1}|x_{1},s;\\theta \\right)\n",
    "f\\left( s|x;\\theta_{S}\\right)\n",
    "$$\n",
    "\n",
    "where $\\theta $ includes both $\\theta_{M}$ and $\\theta_{S}$\n",
    "\n",
    "We cannot use full maximum likelihood\n",
    "- since $y_{1}$ is only observed when $s=1$\n",
    "- we cannot identify $f\\left( y_{1},s|x;\\theta \\right) $ \n",
    "- because $f\\left( y_{1}|x,s=0;\\theta \\right) $ is not identified.\n",
    "- Instead, we will use *partial likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood models: Partial Likelihood\n",
    "\n",
    "Partial Likelihood is maximized at $\\theta_{0}$ (can be shown)\n",
    "\n",
    "$$\n",
    "\\theta_{0}=\\arg \\max_{\\theta} E\\left( s\\cdot \\ln \\left[ f\\left(y_{1}|x_{1},s;\\theta \\right) \\right] \n",
    "+ \\ln \\left[ f\\left( s|x;\\theta_{S}\\right) \\right] \\right) \n",
    "$$\n",
    "- Hence we can disregard the unidentified part of the likelihood\n",
    "\n",
    "We can also show that\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\theta_{M_0} &=&\\arg \\max_{\\theta_{M}}E\\left( s\\cdot \\ln \\left[ f\\left(\n",
    "y_{1}|x_{1},s;\\theta_{M},\\theta_{S_0}\\right) \\right] \\right) \\\\\n",
    "\\theta_{S_0} &=&\\arg \\max_{\\theta_{S}}E\\left( \\ln \\left[ f\\left( s|x;\\theta\n",
    "_{S}\\right) \\right] \\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Parameters $\\theta_{S_0}$ are not restricted to depend on parameters in the structural equation\n",
    "- Estimation can be divided into a two-step procedure (advantageous numerically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1: Normal regression with probit selection\n",
    "\n",
    "Model\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y_{1} &=&x_{1}^{\\prime }\\beta_{1}+u_{1} \\\\\n",
    "s &=&1\\left( x^{\\prime }\\delta_{2}+v_{2}>0\\right)\\\\\n",
    "\\left( \n",
    "\\begin{array}{c}\n",
    "u_{1} \\\\ \n",
    "v_{2}\n",
    "\\end{array}\n",
    "\\right) &\\sim &MVN\\left( \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\ \n",
    "0\n",
    "\\end{array}\n",
    "\\right) ,\\left[ \n",
    "\\begin{array}{cc}\n",
    "\\sigma_{1}^{2} & \\sigma_{12} \\\\ \n",
    "\\sigma_{12} & 1\n",
    "\\end{array}\n",
    "\\right] \\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "- This is the ML version of the linear regression model with probit\n",
    "selection (ML analogue to Heckit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Density of y conditional on s\n",
    "\n",
    "Use Bayes rule to decompe joint density\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "f\\left( y_{1}|x,s;\\theta \\right) &=&f\\left( y_{1},s|x_{1};\\theta \\right)\n",
    "/f\\left( s|x;\\theta \\right)\\\\\n",
    "&=&f\\left( s|y_{1},x;\\theta \\right) f\\left( y_{1}|x;\\theta \\right) /f\\left(\n",
    "s|x;\\theta \\right) \n",
    "\\end{eqnarray*}\n",
    "where \n",
    "$$\n",
    "f\\left( s|x;\\delta_{2}\\right) =\\Phi \\left( x^{\\prime }\\delta_{2}\\right)^{s}\\left( 1-\\Phi \\left( x^{\\prime }\\delta_{2}\\right) ^{1-s}\\right) \n",
    "\\quad [\\text{as before}]\n",
    "$$\n",
    "and \n",
    "$$\n",
    "f\\left( y_{1}|x;\\theta \\right) =\\frac{1}{\\sigma_{1}}\\phi \\left( \\frac{\n",
    "y_{1}-x_{1}^{\\prime }\\beta_{1}}{\\sigma_{1}}\\right) \n",
    "$$\n",
    "still need to derive $f\\left(s|y_{1},x;\\theta \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Density of s conditional on $y_1$ and $x$\n",
    "We can write\n",
    "\n",
    "$$\n",
    "s=1\\left( x^{\\prime }\\delta_{2}+v_{2}>0\\right) \n",
    "$$\n",
    "where\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "v_{2} &=&\\theta u_{1}+e_{1}\\text{, }\\theta =\\frac{\\sigma_{12}}{\\sigma_{1}^2}\\\\\n",
    "&=&\\frac{\\sigma_{12}}{\\sigma_{1}^2}\\left( y_{1}-x_{1}^{\\prime }\\beta_{1}\\right) +e_{1}\\\\\n",
    "e_{1} &\\sim &N\\left( 0,1-\\frac{\\sigma_{12}^{2}}{\\sigma_{1}^{2}}\\right) \n",
    "\\end{eqnarray*}\n",
    "Then\n",
    "$$\n",
    "f\\left( s=1|y_{1},x;\\theta \\right) =\\Phi \\left( \\frac{x^{\\prime }\\delta_{2}+\n",
    "\\frac{\\sigma_{12}}{\\sigma_{1}^2}\\left( y_{1}-x_{1}^{\\prime }\\beta_{1}\\right) }{\\sqrt{1-\\frac{\\sigma_{12}^{2}}{\\sigma_{1}^{2}}}}\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regression model with probit selection: Likelihood function\n",
    "\n",
    "The partial likelihood is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\ell_{PL}\\left( \\beta_{1},\\gamma_{1},\\delta_{2}\\right) \\\\\n",
    "&=&\\sum_{i=1}^{N}s_{i}\\cdot \\ln \\left[ f\\left( y_{1i}|x_{1i},s_{i}=1;\\beta\n",
    "_{1},\\gamma_{1},\\delta_{2}\\right) \\right] +\\ln \\left[ f\\left(\n",
    "s_{i}|x_{i};\\delta_{2}\\right) \\right]\\\\\n",
    "&=&\\sum_{i=1}^{N_{0}}\\ln \\left[ \\Phi \\left( \\frac{x^{\\prime }\\delta_{2}+\n",
    "\\frac{\\sigma_{12}}{\\sigma_{1}}\\left( y_{1}-x_{1}^{\\prime }\\beta\n",
    "_{1}\\right) }{\\sqrt{1-\\frac{\\sigma_{12}^{2}}{\\sigma_{1}^{2}}}}\\right) \n",
    "\\frac{1}{\\sigma_{1}}\\phi \\left( \\frac{y_{1}-x_{1}^{\\prime }\\beta_{1}}{\n",
    "\\sigma_{1}}\\right) \\right] \\\\\n",
    "&&+\\sum_{i=N_{0}+1}^{N}\\ln \\left[ 1-\\Phi \\left( x^{\\prime }\\delta\n",
    "_{2}\\right) \\right] \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "- where there are $N_{0}$ observations where we don't see $y_{1}$ and $N_{1}$ observations where we do\n",
    "- $N_{0}+N_{1}=N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2: Probit with probit selection\n",
    "Model\n",
    "\\begin{eqnarray*}\n",
    "y_{1} &=&1\\left(x_{1}^{\\prime }\\beta_{1}+u_{1}>0\\right) \\\\\n",
    "s &=&1\\left( x_{2}^{\\prime }\\delta_{2}+v_{2}>0\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\left( \n",
    "\\begin{array}{c}\n",
    "u_{1} \\\\ \n",
    "v_{2}\n",
    "\\end{array}\n",
    "\\right) \\sim MVN\\left( \\left( \n",
    "\\begin{array}{c}\n",
    "0 \\\\ \n",
    "0\n",
    "\\end{array}\n",
    "\\right) ,\\left[ \n",
    "\\begin{array}{cc}\n",
    "1 & \\gamma_{1} \\\\ \n",
    "\\gamma_{1} & 1\n",
    "\\end{array}\n",
    "\\right] \\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joint density of y and s\n",
    "For Probit with Probit selection we need to compute: $f\\left( y_{1}|x_{1},s;\\theta \\right) $ and $f\\left(s|x;\\theta_{S}\\right) $ \n",
    "\n",
    "- The latter is easy \n",
    "$$\n",
    "f\\left( s|x\\right) =\\Phi \\left( x^{\\prime }\\delta_{2}\\right) ^{s}\\left(\n",
    "1-\\Phi \\left( x^{\\prime }\\delta_{2}\\right) \\right) ^{1-s}\n",
    "$$\n",
    "\n",
    "- But the density for $y_{1}$ given section is more involved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional density for y given selection\n",
    "\n",
    "Conditional response probability\n",
    "\n",
    "$$\n",
    "f\\left( y_{1}=1|s=1,x_{1}\\right) =E_{v_{2}}\\left \\{ \\left[ \\Phi \\left(\\left( x_{1}^{\\prime }\\beta_{1}+\\gamma_{1}v_{2}\\right) /\\sqrt{1-\\gamma_{1}^{2}}\\right) \\right] \\left \\vert s=1\\right. \\right \\} \n",
    "$$\n",
    "\n",
    "Recall that $\\frac{\\phi \\left( v_{2}\\right) }{\\Phi \\left( x^{\\prime }\\delta_{2}\\right) }$ is the density for the truncated normal $v_{2}|x,s=1$ or\n",
    "equivalently $v_{2}|x,v_{2}>-x^{\\prime }\\delta_{2}$\n",
    "\n",
    "$$\n",
    "f\\left( y_{1}=1|s=1,x_{1}\\right) = \\int_{-x^{\\prime }\\delta_{2}}^{\\infty }\\Phi \\left( \\frac{x_{1}^{\\prime\n",
    "}\\beta_{1}+\\gamma_{1}v_{2}}{\\sqrt{1-\\gamma_{1}^{2}}}\\right) \\frac{\\phi\n",
    "\\left( v_{2}\\right) }{\\Phi \\left( x^{\\prime }\\delta_{2}\\right) }dv_{2}\n",
    "$$\n",
    "\n",
    "Conditional density\n",
    "\n",
    "$$\n",
    "f\\left( y_{1}|s=1,x_{1}\\right) =f\\left( y_{1}=1|s=1,x_{1}\\right)\n",
    "^{y_{1}}\\left( 1-f\\left( y_{1}=1|s=1,x_{1}\\right) \\right) ^{1-y_{1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probit with probit selection: Likelihood\n",
    "\n",
    "The partial likelihood is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\ell_{PL}\\left( \\beta_{1},\\gamma_{1},\\delta_{2}\\right) \\\\\n",
    "&=&\\sum_{i=1}^{N}s_{i}\\cdot \\ln \\left[ f\\left( y_{1i}|x_{1i},s_{i}=1;\\beta_{1},\\gamma_{1},\\delta_{2}\\right) \\right] +\\ln \\left[ f\\left(\n",
    "s_{i}|x_{i};\\delta_{2}\\right) \\right] \n",
    "\\end{eqnarray*}\n",
    "where\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "f\\left( s|x\\right) &=&\\Phi \\left( x^{\\prime }\\delta_{2}\\right) ^{s}\\left(\n",
    "1-\\Phi \\left( x^{\\prime }\\delta_{2}\\right) ^{1-s}\\right) \\\\\n",
    "f\\left( y_{1}|s=1,x_{1}\\right) &=&f\\left( y_{1}=1|s=1,x_{1}\\right) ^{y_{1}} \\\\\n",
    "&&\\left( 1-f\\left( y_{1}=1|s=1,x_{1}\\right) \\right) ^{1-y_{1}} \\\\\n",
    "f\\left( y_{1}=1|s=1,x_{1}\\right) &=&\\int_{-x^{\\prime }\\delta_{2}}^{\\infty\n",
    "}\\Phi \\left( \\frac{x_{1}^{\\prime }\\beta_{1}+\\gamma_{1}v_{2}}{\\sqrt{1-\\gamma_{1}^{2}}}\\right) \\frac{\\phi \\left( v_{2}\\right) }{\\Phi \\left(x^{\\prime }\\delta_{2}\\right) }dv_{2}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood models: Pros and cons\n",
    "\n",
    "MLE approach: Pros and cons\n",
    "\n",
    "Advantage:\n",
    "\t- Robust covariance matrix from partial ML is valid (no generated regressors).\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "- Stronger assumptions (but testable)\n",
    "- Numerically more challenging than Heckit\n",
    "- More Greek letters to juggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Non-Parametric Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-Parametric Identification\n",
    "Nonparametric Identification\n",
    "- **Not required knowledge for the exam**\n",
    "- Material cannot be found in Wooldridge (2010), but draws heavily on Manski (1995).\n",
    "- Hence, this is not required reading for the exam.\n",
    "- But required for your intellectual development!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-Parametric Identification\n",
    "Observables:\n",
    "- Let a random sample of $\\left( y,x,s\\right) $ be drawn from the population.\n",
    "- We observe all realizations of $\\left( x,s\\right) $ but we only $y$ when $s=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-Parametric Identification\n",
    "\n",
    "The sampling does not identify $E\\left( y|x\\right)$. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "E\\left( y|x\\right) &=&E\\left( y|x,s=1\\right) \\Pr \\left( s=1|x\\right)\n",
    "\\label{Manski mean} \\\\\n",
    "&&+E\\left( y|x,s=0\\right) \\Pr \\left( s=0|x\\right) \\notag\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "**The sample process identifies** \n",
    "- the selection probability $\\Pr \\left( s=1|x\\right) $,\n",
    "- the censoring probability $\\Pr \\left( s=0|x\\right) $ \n",
    "- the conditional expectation conditional on selection, that is $E\\left(y|x,s=1\\right) $. \n",
    "\n",
    "BUT: sampling process is completely uninformative about $E\\left(y|x,s=0\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-Parametric Identification\n",
    "\n",
    "**Sampling process does not identify $\\Pr \\left( y\\leq c|x\\right) $**\n",
    "\n",
    "To see this use the law of total probability\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Pr \\left( y\\leq c|x\\right) &=&\\Pr \\left( y\\leq c|x,s=1\\right) \\Pr \\left(\n",
    "s=1|x\\right)  \\label{Manski prob} \\\\\n",
    "&&+\\Pr \\left( y\\leq c|x,s=0\\right) \\Pr \\left( s=0|x\\right) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Sampling process provide no information about $\\Pr \\left( y\\leq c|x,s=0\\right) $\n",
    "- Without further knowledge or assumptions we cannot make bounds on $E\\left( y|x\\right)$ or $\\Pr \\left( y\\leq c|x\\right)$\n",
    "\n",
    " (see the prev. slide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-parametric identification under exogenous selection\n",
    "\n",
    "- Exogenous selection\n",
    "\n",
    "$$ \\Pr \\left( y\\leq c|x,s=1\\right) =\\Pr \\left( y\\leq c|x,s=0\\right) $$\n",
    "\n",
    "- implies that\n",
    "\n",
    "$$ \\Pr \\left( y\\leq c|x\\right) =\\Pr \\left( y\\leq c|x,s=1\\right) $$\n",
    "\n",
    "- But exogenous selection is the same as assuming we do not have a selection problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonparametric Bounds (set identification)\n",
    "\n",
    "Recall \n",
    "\\begin{eqnarray*}\n",
    "\\Pr \\left( y\\leq c|x\\right) &=&\\Pr \\left( y\\leq c|x,s=1\\right) \\Pr \\left(s=1|x\\right) \\\\\n",
    "&&+\\Pr \\left( y\\leq c|x,s=0\\right) \\Pr \\left( s=0|x\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Nonparametric bounds:  Note that by definition $0\\leq \\Pr \\left( y\\leq c|x,s=0\\right) \\leq 1$ \n",
    "\n",
    "- We can put bounds on $\\Pr \\left( y\\leq c|x\\right) $\n",
    "- Lower bound\n",
    "$$\n",
    "\\Pr \\left( y\\leq c|x\\right) \\geq \\Pr \\left( y\\leq c|x,s=1\\right) \\Pr \\left(\n",
    "s=1|x\\right) \n",
    "$$\n",
    "\n",
    "- upper bound\n",
    "\\begin{eqnarray*}\n",
    "\\Pr \\left( y\\leq c|x\\right)  &\\leq &\\Pr \\left( y\\leq c|x,s=1\\right) \\Pr\\left( s=1|x\\right)  \\\\\n",
    "&&+\\Pr \\left( s=0|x\\right) \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Bounds are tight when $\\Pr \\left( s=1|x\\right)$ is close to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonparametric bounds: Binary exclusive restriction\n",
    "\n",
    "Suppose we have a **binary exclusion restriction** $z=\\left \\{z_{1},z_{2}\\right \\}$\n",
    "\n",
    "New bounds:\n",
    "\\begin{eqnarray*}\n",
    "&&\\underset{j=1,2}{\\max }\\left \\{ \\Pr \\left( y\\leq c|x,z=z_{j},s=1\\right) \\Pr\n",
    "\\left( s=1|x,z=z_{j}\\right) \\right \\}  \\\\\n",
    "&\\leq &\\Pr \\left( y\\leq c|x\\right)  \\\\\n",
    "&\\leq &\\underset{j=1,2}{\\min }\\left \\{ \n",
    "\\begin{array}{c}\n",
    "\\Pr \\left( y\\leq c|x,z=z_{j},s=1\\right) \\Pr \\left( s=1|x,z=z_{j}\\right)  \\\\ \n",
    "+\\Pr \\left( s=0|x,z=z_{j}\\right) \n",
    "\\end{array}\n",
    "\\right \\} \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "- more tight than without exclusion restrictions if $z$ has an effect on $s$\n",
    "- but still we cannot point identify $\\Pr \\left( y\\leq c|x\\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Point Identification Heckman's model\n",
    "\n",
    "What are the assumptions in the Heckman sample selection model that give point identification?\n",
    "- Errors in the selection equation and the equation of interest are correlated (either positively or negatively).\n",
    "- Exclusion restriction does not appear in the equation of interest, but only in the selection equation.\n",
    "- The normality assumption (if no exclusion restrictions)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1602643870.398518,
  "filename": "38_optimization.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "Econometrics B #13"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
